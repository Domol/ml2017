{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L10\n",
    "\n",
    "Sieci neuronowe - ciąg dalszy.\n",
    "\n",
    "Na tych (i prawdopodobnie też następnych) ćwiczeniach omówimy:\n",
    "* wielowarstwową sieć neuronową,\n",
    "* funkcje aktywacji: ReLU, sigmoid,\n",
    "* funkcja kosztu (cross-entropy) vs metryka,\n",
    "* backpropagation,\n",
    "* SGD oraz Momentum,\n",
    "* problem vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://distill.pub/2017/momentum/\n",
    "https://en.wikipedia.org/wiki/Backpropagation\n",
    "https://en.wikipedia.org/wiki/Delta_rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[rysunek sieci]\n",
    "[rysunek sieci z funkcją kosztu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dlaczego będziemy liczyć pochodne - motywacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU jako funkcja:\n",
    "* $\\mathbb{R} \\to \\mathbb{R}$\n",
    "* $\\mathbb{R}^N \\to \\mathbb{R}^N$\n",
    "\n",
    "Pochodna w obu przypadkach. Dwa wykresy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid jako funkcja:\n",
    "* $\\mathbb{R} \\to \\mathbb{R}$\n",
    "* $\\mathbb{R}^N \\to \\mathbb{R}^N$\n",
    "\n",
    "sprytny zapis\n",
    "\n",
    "Pochodna w obu przypadkach. Dwa wykresy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-entropy jeszcze raz:\n",
    "* 2 przypadki: y==0, y==1\n",
    "* pochodna\n",
    "\n",
    "c-e vs accuracy\n",
    "\n",
    "można liczyć też pochodną po y_true, ale tego nie robimy - dlaczego?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neuron z jedynką bez/z aktywacją, wzór i jego pochodna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backpropagation\n",
    "\n",
    "rysunek pojedynczego neuronu - dostaje z góry dużo informacji na temat tego, jak powinien się zmienić, rozdziela to na swoje wagi oraz inputy\n",
    "\n",
    "rysunek w wersji bez aktywacji oraz sama aktywacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-3c85f297ee1d>, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3c85f297ee1d>\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    def backward_pass(self, y_true, y_pred):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# dużo kodu, który można kawałkami uzupełniać i stosunkowo wcześnie działa\n",
    "\n",
    "class Layer():\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        pass\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        raise NotImplementedError()\n",
    "        # return output\n",
    "    \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        # weights = self.weights\n",
    "        raise NotImplementedError()\n",
    "        # return input_grad, weights_grad\n",
    "\n",
    "    def update_weights(self, delta_weights): # activation layers?\n",
    "        self.weights += delta_weights # or sth like that\n",
    "\n",
    "    def reshape_weights(self, vector_of_weights):\n",
    "        return matrix_of_weights\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # gaussian\n",
    "        #\n",
    "\n",
    "class ReLU(Layer):\n",
    "    pass\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    pass\n",
    "\n",
    "class Dense(Layer):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Optimizer():\n",
    "    pass\n",
    "\n",
    "class SGD(Optimizer):\n",
    "\n",
    "    # ja kodzę\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        pass\n",
    "\n",
    "    def calculate_delta(self, grad):\n",
    "        pass\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "\n",
    "    def __init__(self, alpha, beta):\n",
    "        pass\n",
    "\n",
    "    def calculate_delta(self, grad):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # ja kodzę - y_pred rzeczywiste, threshold 0,5\n",
    "    pass\n",
    "\n",
    "class Loss():\n",
    "    pass\n",
    "\n",
    "class MSE(Loss):\n",
    "    # ja kodzę\n",
    "\n",
    "class Crossentropy(Loss):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward_pass(self, y_true, y_pred):\n",
    "        # return cost\n",
    "    \n",
    "    def backward_pass(self, y_true, y_pred):\n",
    "        raise NotImplementedError()\n",
    "        # return y_pred_grad\n",
    "\n",
    "        \n",
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, params):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, nb_epochs, batch_size):\n",
    "        # ja kodzę\n",
    "        \n",
    "        pass\n",
    "        return history # ?\n",
    "\n",
    "    def fit_on_batch(self, X_batch, y_batch):\n",
    "        pass\n",
    "\n",
    "        # forward pass\n",
    "        _o = None # list of vectors, including input, update after each forward pass\n",
    "        # backward pass\n",
    "        _grads = None # list of vectors, update after each backward pass\n",
    "        # concat gradients\n",
    "        # use Optimizer to calculate deltas\n",
    "        # split deltas\n",
    "        # update layers\n",
    "        # some_stats = (concat_gradients, concat_o, ...)\n",
    "\n",
    "\n",
    "        return some_stats\n",
    "\n",
    "    def add(self, layer):\n",
    "        # check sizes\n",
    "        # ja kodzę\n",
    "        pass\n",
    "\n",
    "    def compile(self, loss, optimizer, metrics):\n",
    "        # loss - Loss\n",
    "        # optimizer - Optimizer\n",
    "        # metrics - list of f(y_true, y_pred)\n",
    "        # ja kodzę\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        # ja kodzę - \n",
    "        pass\n",
    "\n",
    "    def _forward_pass(self, X_batch):\n",
    "        pass\n",
    "        # ja kodzę\n",
    "        return list_of_o\n",
    "\n",
    "    def _backward_pass(self, list_of_o):\n",
    "        pass\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11339, 784)\n",
      "(1850, 784)\n"
     ]
    }
   ],
   "source": [
    "# Two-class MNIST \n",
    "\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "d1 = 5\n",
    "d2 = 6\n",
    "\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "\n",
    "X_train = (mnist_x_train.astype('float32') / 255.).reshape((len(mnist_x_train), np.prod(mnist_x_train.shape[1:])))\n",
    "y_train = mnist_y_train\n",
    "X_test = (mnist_x_test.astype('float32') / 255.).reshape((len(mnist_x_test), np.prod(mnist_x_test.shape[1:])))\n",
    "y_test = mnist_y_test\n",
    "\n",
    "X_train = X_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train = y_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "X_test = X_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test = y_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zadania\n",
    "\n",
    "1 - wypełnij warstwy\n",
    "\n",
    "2 - wypełnij fit_on_batch\n",
    "\n",
    "3 - wypełnij _backward_pass\n",
    "\n",
    "4 - naucz dwuwarstwową sieć na dwuklasowym mniście (regresja do numeru klasy, zły pomysł, ale już mamy MSE), \n",
    "\n",
    "5 - wypełnij crossentropy loss\n",
    "\n",
    "6 - wypełnij momentum\n",
    "\n",
    "7 - vanishing gradient - naucz dwuwarstwową sieć na dwuklasowym mniście reportując normy gradientów; to samo dla głębszej sieci\n",
    "\n",
    "8 - zepsuć inicjalizację wag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
