{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11339, 784)\n",
      "(1850, 784)\n"
     ]
    }
   ],
   "source": [
    "# Two-class MNIST \n",
    "\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "d1 = 5\n",
    "d2 = 6\n",
    "\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "\n",
    "X_train = (mnist_x_train.astype('float32') / 255.).reshape((len(mnist_x_train), np.prod(mnist_x_train.shape[1:])))\n",
    "y_train = mnist_y_train\n",
    "X_test = (mnist_x_test.astype('float32') / 255.).reshape((len(mnist_x_test), np.prod(mnist_x_test.shape[1:])))\n",
    "y_test = mnist_y_test\n",
    "\n",
    "X_train = X_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train = y_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train[y_train==d1] = 0\n",
    "y_train[y_train==d2] = 1\n",
    "X_test = X_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test = y_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test[y_test==d1] = 0\n",
    "y_test[y_test==d2] = 1\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1 [5 pkt] ok?\n",
    "\n",
    "Uzupełnij metody forward_pass oraz backward_pass w klasach ReLU, Sigmoid i Dense. Metoda forward_pass ma przyjmować batch inputów i zwracać batch outputów. Metoda backward_pass ma przyjmować batch inputów oraz batch pochodnych cząstkowych outputów i zwracać batch pochodnych cząstkowych inputów oraz wektor (**nie batch**) pochodnych cząstkowych wag. Jeśli wagi przechowujemy w macierzy dwuwymiarowej, to możemy najpierw policzyć pochodne cząstkowe w macierzy o takim samym kształcie, a następnie np. użyć .flat. \n",
    "\n",
    "Uwaga: dla warstw bez wag należy zwrócić None.\n",
    "\n",
    "## Ćwiczenie 2 [4 pkt] ok ?\n",
    "\n",
    "Uzupełnij metodę _forward_pass klasy Network. Metoda ta ma przyjmować batch inputów (X) i zwracać dwie rzeczy:\n",
    "* inps - lista batchów inputów dla każdej warstwy w sieci (włącznie z X); te wartości będziemy używali w metodzie _backward_pass\n",
    "* output - batch outputów z sieci (czyli $\\mathbf{\\hat y}$); output **nie** powinien być ostatnim elementem inps.\n",
    "\n",
    "## Ćwiczenie 3 [5 pkt] ok ?\n",
    "\n",
    "Uzupełnij metodę _backward_pass klasy Network. Zwróć uwagę, że pochodna funkcji kosztu po neuronach ostatniej warstwy jest już liczona w metodzie _fit_on_batch. Metoda ma zwracać listę layer_grads, której elementy to wektory pochodnych cząstkowych funkcji kosztu po kolejnych warstwach (zwrócone przez metodę Layer.backward_pass). Kolejność wektorów w tej liście ma być zgodna z kolejnością warstw w sieci.\n",
    "\n",
    "## Ćwiczenie 4 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą i aktywacją Sigmoid na powyższych danych (dwuklasowy MNIST). Użyj MSE jako funkcji kosztu (oznacza to regresję do numeru klasy, co jest złym pomysłem, ale póki nie mamy klasy Crossentropy musi nam to wystarczyć). Użyj GD. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 5 [3 pkt]\n",
    "Uzupełnić klasę Crossentropy, wzorując się na klasie MSE.\n",
    "\n",
    "## Ćwiczenie  6 [3 pkt]\n",
    "Uzupełnić klasę Momentum, wzorując się na klasie GD. Wzory można znaleźć tutaj: http://distill.pub/2017/momentum/\n",
    "\n",
    "## Ćwiczenie 7 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą. Rozważ dwa przypadki: aktywację ReLU oraz Sigmoid. Czy jest sens używać ReLU jako ostatnią warstwę? Użyj Crossentropy jako funkcji kosztu. Użyj Momentum. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 8 [6 pkt]\n",
    "Vanishing gradient.\n",
    "\n",
    "Zadanie polega na zbadaniu zjawiska *vanishing gradient* w głębokich sieciach. Należy zmodyfikować kod warstwy Dense i dodać monitorowanie **normy euklidesowej** wektora delta_weights. Każdą warstwę Dense w trenowanej sieci należy monitorować oddzielnie. Po każdym wywołaniu metody fit_on_batch każdy z monitorów powinien zapamiętać nową normę. Po nauczeniu sieci dla każdej warstwy należy narysować wykres: poziomo - numer wywołania fit_on_batch, pionowo - norma delta_weights. Im niżej znajduje się warstwa Dense, tym silniej będzie zachodziło zjawisko *vanishing gradient*.\n",
    "\n",
    "Naucz dwuwarstwową sieć z aktywacjami Sigmoid, reportując normy delta_weights. Powtórz to dla głębszej sieci (np. 6-10 warstw).\n",
    "\n",
    "## Ćwiczenie 9 [4 pkt]\n",
    "Przetestować kod z ćwiczenia 7. (dwuwarstwowa sieć) stosując inne inicjalizacje wag w warstwach Dense. Napisać własną inicjalizację wag, która sprawi, że sieć niczego się nie nauczy (init='stupid').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warstwy\n",
    "\n",
    "class Layer():\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        # return output\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, input, output_grad):\n",
    "        # return input_grad, weight_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_weights(self, delta_weights):\n",
    "        pass\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        return None\n",
    "\n",
    "class ReLU(Layer):\n",
    "    \n",
    "    def forward_pass(self, input_):\n",
    "        return np.maximum(np.zeros(input_.shape), input_)\n",
    "        \n",
    "    def backward_pass(self, input_, output_grad):\n",
    "        # Ćwiczenie 1\n",
    "        fbool = np.vectorize(lambda x: int(x>0))\n",
    "        ones = fbool(input_)\n",
    "#         print np.multiply(ones, output_grad), None\n",
    "        return np.multiply(ones, output_grad), None\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    sigma = np.vectorize(lambda x: 1/float(1+np.exp(-x)))\n",
    "    def forward_pass(self, input):\n",
    "        # Ćwiczenie 1\n",
    "#         return self.sigma(np.array(map(lambda f: [np.float128(x) for x in f], input)))\n",
    "        return self.sigma(input)\n",
    "\n",
    "    def backward_pass(self, input_, output_grad):\n",
    "        # Ćwiczenie 1\n",
    "        sigm_input = self.sigma(input_)\n",
    "#         print reduce(np.multiply,\n",
    "#             [output_grad,\n",
    "#             sigm_input,\n",
    "#             np.subtract(np.ones(sigm_input.shape), sigm_input)]), None\n",
    "        return reduce(np.multiply,\n",
    "            [output_grad,\n",
    "            sigm_input,\n",
    "            np.subtract(np.ones(sigm_input.shape), sigm_input)]), None\n",
    "\n",
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_size, output_size, init = 'gaussian'):\n",
    "        input_size += 1\n",
    "        if init == 'zeros':\n",
    "            self.weights = np.zeros((input_size, output_size))\n",
    "        elif init == 'gaussian':\n",
    "            self.weights = np.random.normal(\n",
    "                0.,\n",
    "                2. / (input_size + output_size),\n",
    "                (input_size, output_size)\n",
    "            )\n",
    "        elif init == 'stupid':\n",
    "            # Ćwiczenie 8\n",
    "            raise NotImplementedError()            \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        self.weights = np.asmatrix(self.weights)\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        # Ćwiczenie 1\n",
    "        x = np.hstack((np.ones((input.shape[0], 1)), input))\n",
    "        return x*self.weights\n",
    "    \n",
    "    \n",
    "    def backward_pass(self, input_, output_grad): # TU MOZE BYC BLAD\n",
    "        # Ćwiczenie 1\n",
    "        input_ = np.hstack((np.ones((input_.shape[0], 1)), input_))\n",
    "        \n",
    "        dLdW = input_.T*output_grad\n",
    "        dLdX = self.weights * output_grad.T\n",
    "        \n",
    "        input_grad = dLdX.T[:,1:]\n",
    "        weight_grad = np.array(dLdW.flat)\n",
    "#         print input_grad, weight_grad\n",
    "        return input_grad, weight_grad\n",
    "\n",
    "    def update_weights(self, delta_weights):\n",
    "        # Ćwiczenie 7 - monitorowanie normy wektora delta_weights\n",
    "#         print delta_weights.shape, self.weights.shape\n",
    "        self.weights += delta_weights.reshape(self.weights.shape)\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        base = evaluate_loss()\n",
    "        grad = []\n",
    "        for (x, y), w in np.ndenumerate(self.weights):\n",
    "            self.weights[x, y] = w + 0.0001\n",
    "            changed = evaluate_loss()\n",
    "            grad.append(10000. * (changed - base))\n",
    "            self.weights[x, y] = w\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "class Optimizer():\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class GD(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        return -self.learning_rate * grad\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.prev_z = 0\n",
    "        \n",
    "    def calculate_z(self, grad):\n",
    "        z = self.beta*self.prev_z + grad\n",
    "        self.prev_z = z\n",
    "        return z\n",
    "    \n",
    "    def calculate_deltas(self, grad):\n",
    "        return -self.alpha*self.calculate_z(grad)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcje kosztu\n",
    "\n",
    "class Loss():\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        # return cost\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        # return y_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MSE(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        print y, t\n",
    "        return np.average(0.5 * np.square(y - t))\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        return (y - t) / y.size\n",
    "\n",
    "class Crossentropy(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        print 'y:', y\n",
    "        res = 0.0\n",
    "        len_ = len(t)\n",
    "        for i in range(len_):\n",
    "            t_i = t[i]\n",
    "            y_i = y[i]\n",
    "            res += -t_i*np.log(y_iqq) - (1 - t_i)*np.log(1 - y_i)\n",
    "        res /= len_\n",
    "        print res\n",
    "        return res\n",
    "    def backward_pass(self, y, t):\n",
    "        return ((np.divide(np.ones(t.shape) - t, np.ones(y.shape) - y)) - np.divide(t,y))/t.size\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(\\mathbf{y_{true}}, \\mathbf{y_{pred}}) = L(\\mathbf{t}, \\mathbf{\\hat y}) =\\frac{1}{\\mathrm{len}(\\mathbf{t})} \\sum_{j=1}^{\\mathrm{len}(\\mathbf{t})} - \\mathbf{t}_j \\log \\mathbf{\\hat y}_j - (1 - \\mathbf{t}_j) \\log (1 - \\mathbf{\\hat y}_j ) $$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{\\hat y}} = \\frac{1}{\\mathrm{len}(\\mathbf{t})}(-\\frac{\\mathbf{t}}{\\mathbf{\\hat y}} + \\frac{1-\\mathbf{t}}{1-\\mathbf{\\hat y}})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    def __init__(self, loss, optimizer, metrics = []):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, X, t, epochs, batch_size=256, print_stats=False):\n",
    "        X = np.array(X)\n",
    "        t = np.array(t)\n",
    "        X = X.reshape(len(X), -1)\n",
    "        t = t.reshape(len(t), -1)\n",
    "        if X.shape[0] != t.shape[0]:\n",
    "            raise ValueError(\"Array sizes don't match\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if print_stats:\n",
    "                print(\"Epoch %d\" % (epoch+1))\n",
    "                print(\"    -> batch size: %d\" % batch_size)\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(X)\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(t)\n",
    "            pos = 0\n",
    "            while pos < len(X):\n",
    "                batch_X = X[pos:pos+batch_size]\n",
    "                batch_t = t[pos:pos+batch_size]\n",
    "                self._fit_on_batch(batch_X, batch_t)\n",
    "                pos += batch_size\n",
    "            if print_stats:\n",
    "                _, y = self._forward_pass(X)\n",
    "                print y,t\n",
    "                l = self.loss.forward_pass(y, t)\n",
    "                print l\n",
    "                print(\"    -> loss: %f\" % l)\n",
    "                for m in self.metrics:\n",
    "                    print(\"    -> %s: %f\" % (m.__name__, m(y, t)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        inps, out = self._forward_pass(X)\n",
    "        return out\n",
    "\n",
    "    def _fit_on_batch(self, batch_X, batch_t):\n",
    "        inps, out = self._forward_pass(batch_X)\n",
    "        layer_grads = self._backward_pass(\n",
    "            inps,\n",
    "            self.loss.backward_pass(out, batch_t)\n",
    "        )\n",
    "        grad = self._join(layer_grads)\n",
    "        deltas = self.optimizer.calculate_deltas(grad)\n",
    "        for l, d in zip(self.layers, self._split(deltas, layer_grads)):\n",
    "            if not d is None:\n",
    "                l.update_weights(d)\n",
    "\n",
    "    def _join(self, grads):\n",
    "        return np.concatenate([g for g in grads if not g is None])\n",
    "\n",
    "    def _split(self, grads, layer_grads):\n",
    "        out = []\n",
    "        start = 0\n",
    "        for l in layer_grads:\n",
    "            if l is None:\n",
    "                out.append(None)\n",
    "            else:\n",
    "                out.append(grads[start:start+len(l)])\n",
    "                start += len(l)\n",
    "        return out\n",
    "\n",
    "    def _forward_pass(self, X):\n",
    "        inps = []\n",
    "        output = None\n",
    "\n",
    "        # Ćwiczenie 2\n",
    "        inps = [X]\n",
    "        output = None\n",
    "        for i in range(0,len(self.layers)):\n",
    "            inps.append(self.layers[i].forward_pass(inps[i]))\n",
    "        output = inps.pop()\n",
    "        print output\n",
    "        return inps, output\n",
    "\n",
    "    def _backward_pass(self, inps, grad):\n",
    "        # Ćwiczenie 3\n",
    "        layer_grads = []\n",
    "        length = len(inps)\n",
    "        out_grad = grad\n",
    "        for i in range(1, len(self.layers)+1):\n",
    "            lr_ind = length - i\n",
    "#             print layer_grads\n",
    "#             print 'Layer %s: %s' % (lr_ind, self.layers[lr_ind].__class__)\n",
    "#             print 'inp:', inps[lr_ind]\n",
    "#             print 'grad:', layer_grads[0]\n",
    "            k = self.layers[lr_ind].backward_pass(inps[lr_ind], out_grad)\n",
    "#             print 'K', k\n",
    "            res = k[1]\n",
    "            out_grad = k[0]\n",
    "            layer_grads = [res] + layer_grads\n",
    "        return layer_grads\n",
    "\n",
    "    def _debug_grads(self, X, t):\n",
    "        layer_grads = []\n",
    "        for l in self.layers:\n",
    "            g = l.debug_grad(\n",
    "                lambda: self.loss.forward_pass(self._forward_pass(X)[1], t)\n",
    "            )\n",
    "            if not g is None:\n",
    "                g = np.array(np.array(g).flat)\n",
    "            layer_grads.append(g)\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 11339\n",
      "[[ 0.49977404  0.4981258   0.50551024 ...,  0.49715262  0.50001499\n",
      "   0.49514273]\n",
      " [ 0.50218859  0.49679642  0.502968   ...,  0.49696085  0.50299734\n",
      "   0.49623841]\n",
      " [ 0.50210716  0.49953333  0.50564114 ...,  0.49463684  0.49798808\n",
      "   0.50125438]\n",
      " ..., \n",
      " [ 0.49960296  0.49835554  0.50483702 ...,  0.49738332  0.49717017\n",
      "   0.49502892]\n",
      " [ 0.50039311  0.49714135  0.504833   ...,  0.49578909  0.50100137\n",
      "   0.48996793]\n",
      " [ 0.49967925  0.50166946  0.50041971 ...,  0.4983858   0.50059006\n",
      "   0.49672656]]\n",
      "[[ 0.49977205  0.49812429  0.50550715 ...,  0.4971514   0.50001331\n",
      "   0.49514185]\n",
      " [ 0.50222366  0.49683221  0.5030019  ...,  0.49699678  0.50303273\n",
      "   0.4962749 ]\n",
      " [ 0.5021007   0.49952722  0.50563383 ...,  0.49463099  0.49798192\n",
      "   0.50124879]\n",
      " ..., \n",
      " [ 0.49959186  0.49834495  0.50482454 ...,  0.4973731   0.49715946\n",
      "   0.49501916]\n",
      " [ 0.50042638  0.49717528  0.50486514 ...,  0.49582313  0.50103487\n",
      "   0.49000259]\n",
      " [ 0.49967798  0.50166836  0.50041796 ...,  0.49838481  0.50058891\n",
      "   0.49672583]]\n",
      "[[ 0.49977205  0.49812429  0.50550715 ...,  0.4971514   0.50001331\n",
      "   0.49514185]\n",
      " [ 0.50222366  0.49683221  0.5030019  ...,  0.49699678  0.50303273\n",
      "   0.4962749 ]\n",
      " [ 0.5021007   0.49952722  0.50563383 ...,  0.49463099  0.49798192\n",
      "   0.50124879]\n",
      " ..., \n",
      " [ 0.49959186  0.49834495  0.50482454 ...,  0.4973731   0.49715946\n",
      "   0.49501916]\n",
      " [ 0.50042638  0.49717528  0.50486514 ...,  0.49582313  0.50103487\n",
      "   0.49000259]\n",
      " [ 0.49967798  0.50166836  0.50041796 ...,  0.49838481  0.50058891\n",
      "   0.49672583]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " ..., \n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "[[ 0.49977205  0.49812429  0.50550715 ...,  0.4971514   0.50001331\n",
      "   0.49514185]\n",
      " [ 0.50222366  0.49683221  0.5030019  ...,  0.49699678  0.50303273\n",
      "   0.4962749 ]\n",
      " [ 0.5021007   0.49952722  0.50563383 ...,  0.49463099  0.49798192\n",
      "   0.50124879]\n",
      " ..., \n",
      " [ 0.49959186  0.49834495  0.50482454 ...,  0.4973731   0.49715946\n",
      "   0.49501916]\n",
      " [ 0.50042638  0.49717528  0.50486514 ...,  0.49582313  0.50103487\n",
      "   0.49000259]\n",
      " [ 0.49967798  0.50166836  0.50041796 ...,  0.49838481  0.50058891\n",
      "   0.49672583]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " ..., \n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "0.124982925795\n",
      "    -> loss: 0.124983\n",
      "Epoch 2\n",
      "    -> batch size: 11339\n",
      "[[ 0.49848331  0.50488783  0.50565243 ...,  0.4946396   0.49916841\n",
      "   0.49657586]\n",
      " [ 0.49731821  0.50169978  0.50500787 ...,  0.49927994  0.49702602\n",
      "   0.49760044]\n",
      " [ 0.50257971  0.49403307  0.50173188 ...,  0.49763274  0.50066835\n",
      "   0.49587061]\n",
      " ..., \n",
      " [ 0.49732693  0.49830979  0.50111179 ...,  0.50251665  0.50134905\n",
      "   0.4921752 ]\n",
      " [ 0.49965295  0.50309008  0.50507027 ...,  0.49453757  0.49910943\n",
      "   0.49922658]\n",
      " [ 0.50073543  0.49958341  0.50250255 ...,  0.50093303  0.49868978\n",
      "   0.49544476]]\n",
      "[[ 0.49846758  0.50487243  0.50563559 ...,  0.4946246   0.49915299\n",
      "   0.49656114]\n",
      " [ 0.4973179   0.50169991  0.50500641 ...,  0.49928037  0.49702604\n",
      "   0.49760125]\n",
      " [ 0.50261158  0.4940656   0.50176275 ...,  0.49766539  0.5007005\n",
      "   0.49590375]\n",
      " ..., \n",
      " [ 0.49733031  0.49831358  0.50111418 ...,  0.50252058  0.50135265\n",
      "   0.49217968]\n",
      " [ 0.4996371   0.50307456  0.5050533  ...,  0.49452246  0.49909395\n",
      "   0.49921183]\n",
      " [ 0.50073975  0.49958836  0.50250539 ...,  0.50093836  0.49869456\n",
      "   0.49545068]]\n",
      "[[ 0.49846758  0.50487243  0.50563559 ...,  0.4946246   0.49915299\n",
      "   0.49656114]\n",
      " [ 0.4973179   0.50169991  0.50500641 ...,  0.49928037  0.49702604\n",
      "   0.49760125]\n",
      " [ 0.50261158  0.4940656   0.50176275 ...,  0.49766539  0.5007005\n",
      "   0.49590375]\n",
      " ..., \n",
      " [ 0.49733031  0.49831358  0.50111418 ...,  0.50252058  0.50135265\n",
      "   0.49217968]\n",
      " [ 0.4996371   0.50307456  0.5050533  ...,  0.49452246  0.49909395\n",
      "   0.49921183]\n",
      " [ 0.50073975  0.49958836  0.50250539 ...,  0.50093836  0.49869456\n",
      "   0.49545068]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[ 0.49846758  0.50487243  0.50563559 ...,  0.4946246   0.49915299\n",
      "   0.49656114]\n",
      " [ 0.4973179   0.50169991  0.50500641 ...,  0.49928037  0.49702604\n",
      "   0.49760125]\n",
      " [ 0.50261158  0.4940656   0.50176275 ...,  0.49766539  0.5007005\n",
      "   0.49590375]\n",
      " ..., \n",
      " [ 0.49733031  0.49831358  0.50111418 ...,  0.50252058  0.50135265\n",
      "   0.49217968]\n",
      " [ 0.4996371   0.50307456  0.5050533  ...,  0.49452246  0.49909395\n",
      "   0.49921183]\n",
      " [ 0.50073975  0.49958836  0.50250539 ...,  0.50093836  0.49869456\n",
      "   0.49545068]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "0.124972760857\n",
      "    -> loss: 0.124973\n",
      "Epoch 3\n",
      "    -> batch size: 11339\n",
      "[[ 0.50050539  0.49637615  0.50499615 ...,  0.49728581  0.50236885\n",
      "   0.49491166]\n",
      " [ 0.50151887  0.49651841  0.50327586 ...,  0.49736666  0.50129322\n",
      "   0.49664447]\n",
      " [ 0.5006802   0.50202351  0.50559503 ...,  0.49951178  0.49900415\n",
      "   0.50121017]\n",
      " ..., \n",
      " [ 0.50169328  0.49935717  0.50651136 ...,  0.4995995   0.50086945\n",
      "   0.49758626]\n",
      " [ 0.49962135  0.49969273  0.50530841 ...,  0.49838057  0.50111357\n",
      "   0.4984705 ]\n",
      " [ 0.49981444  0.50245175  0.50851772 ...,  0.49546801  0.49670531\n",
      "   0.49885036]]\n",
      "[[ 0.50055739  0.49642911  0.50504656 ...,  0.49733891  0.50242122\n",
      "   0.49496555]\n",
      " [ 0.50156859  0.49656895  0.50332436 ...,  0.49741727  0.50134323\n",
      "   0.49669571]\n",
      " [ 0.50067943  0.50202313  0.50559332 ...,  0.49951167  0.49900368\n",
      "   0.50121035]\n",
      " ..., \n",
      " [ 0.50173074  0.49939533  0.5065477  ...,  0.49963777  0.50090719\n",
      "   0.49762508]\n",
      " [ 0.49965139  0.49972329  0.50533756 ...,  0.49841122  0.50114379\n",
      "   0.49850156]\n",
      " [ 0.49981899  0.50245688  0.50852078 ...,  0.49547348  0.49671025\n",
      "   0.49885643]]\n",
      "[[ 0.50055739  0.49642911  0.50504656 ...,  0.49733891  0.50242122\n",
      "   0.49496555]\n",
      " [ 0.50156859  0.49656895  0.50332436 ...,  0.49741727  0.50134323\n",
      "   0.49669571]\n",
      " [ 0.50067943  0.50202313  0.50559332 ...,  0.49951167  0.49900368\n",
      "   0.50121035]\n",
      " ..., \n",
      " [ 0.50173074  0.49939533  0.5065477  ...,  0.49963777  0.50090719\n",
      "   0.49762508]\n",
      " [ 0.49965139  0.49972329  0.50533756 ...,  0.49841122  0.50114379\n",
      "   0.49850156]\n",
      " [ 0.49981899  0.50245688  0.50852078 ...,  0.49547348  0.49671025\n",
      "   0.49885643]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " ..., \n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "[[ 0.50055739  0.49642911  0.50504656 ...,  0.49733891  0.50242122\n",
      "   0.49496555]\n",
      " [ 0.50156859  0.49656895  0.50332436 ...,  0.49741727  0.50134323\n",
      "   0.49669571]\n",
      " [ 0.50067943  0.50202313  0.50559332 ...,  0.49951167  0.49900368\n",
      "   0.50121035]\n",
      " ..., \n",
      " [ 0.50173074  0.49939533  0.5065477  ...,  0.49963777  0.50090719\n",
      "   0.49762508]\n",
      " [ 0.49965139  0.49972329  0.50533756 ...,  0.49841122  0.50114379\n",
      "   0.49850156]\n",
      " [ 0.49981899  0.50245688  0.50852078 ...,  0.49547348  0.49671025\n",
      "   0.49885643]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " ..., \n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "0.124962597701\n",
      "    -> loss: 0.124963\n",
      "Epoch 4\n",
      "    -> batch size: 11339\n",
      "[[ 0.50111351  0.49572442  0.5043902  ...,  0.50101688  0.50173808\n",
      "   0.49637723]\n",
      " [ 0.50195741  0.495604    0.50701421 ...,  0.49834686  0.49908241\n",
      "   0.4946942 ]\n",
      " [ 0.5042626   0.49669304  0.50083805 ...,  0.50024841  0.49840538\n",
      "   0.49874049]\n",
      " ..., \n",
      " [ 0.49828716  0.50004019  0.50339803 ...,  0.49744644  0.50120394\n",
      "   0.4992205 ]\n",
      " [ 0.50010982  0.50205389  0.50275305 ...,  0.49963529  0.50108147\n",
      "   0.49903745]\n",
      " [ 0.50344768  0.49814825  0.5041831  ...,  0.49575091  0.50187034\n",
      "   0.4971628 ]]\n",
      "[[ 0.50116181  0.49577353  0.50443727 ...,  0.50106607  0.50178665\n",
      "   0.49642704]\n",
      " [ 0.50199134  0.49563884  0.50704648 ...,  0.49838193  0.49911677\n",
      "   0.49472998]\n",
      " [ 0.50429999  0.49673115  0.50087434 ...,  0.50028663  0.49844313\n",
      "   0.49877933]\n",
      " ..., \n",
      " [ 0.4982971   0.50005042  0.50340729 ...,  0.49745676  0.50121399\n",
      "   0.49923114]\n",
      " [ 0.50011596  0.50206041  0.50275829 ...,  0.49964198  0.50108784\n",
      "   0.49904461]\n",
      " [ 0.5034795   0.49818084  0.50421362 ...,  0.49578372  0.50190255\n",
      "   0.49719618]]\n",
      "[[ 0.50116181  0.49577353  0.50443727 ...,  0.50106607  0.50178665\n",
      "   0.49642704]\n",
      " [ 0.50199134  0.49563884  0.50704648 ...,  0.49838193  0.49911677\n",
      "   0.49472998]\n",
      " [ 0.50429999  0.49673115  0.50087434 ...,  0.50028663  0.49844313\n",
      "   0.49877933]\n",
      " ..., \n",
      " [ 0.4982971   0.50005042  0.50340729 ...,  0.49745676  0.50121399\n",
      "   0.49923114]\n",
      " [ 0.50011596  0.50206041  0.50275829 ...,  0.49964198  0.50108784\n",
      "   0.49904461]\n",
      " [ 0.5034795   0.49818084  0.50421362 ...,  0.49578372  0.50190255\n",
      "   0.49719618]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "[[ 0.50116181  0.49577353  0.50443727 ...,  0.50106607  0.50178665\n",
      "   0.49642704]\n",
      " [ 0.50199134  0.49563884  0.50704648 ...,  0.49838193  0.49911677\n",
      "   0.49472998]\n",
      " [ 0.50429999  0.49673115  0.50087434 ...,  0.50028663  0.49844313\n",
      "   0.49877933]\n",
      " ..., \n",
      " [ 0.4982971   0.50005042  0.50340729 ...,  0.49745676  0.50121399\n",
      "   0.49923114]\n",
      " [ 0.50011596  0.50206041  0.50275829 ...,  0.49964198  0.50108784\n",
      "   0.49904461]\n",
      " [ 0.5034795   0.49818084  0.50421362 ...,  0.49578372  0.50190255\n",
      "   0.49719618]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.124952436326\n",
      "    -> loss: 0.124952\n",
      "Epoch 5\n",
      "    -> batch size: 11339\n",
      "[[ 0.49910466  0.49698961  0.50249892 ...,  0.49906033  0.49915766\n",
      "   0.49365447]\n",
      " [ 0.50359893  0.49621564  0.50720326 ...,  0.49633719  0.50423156\n",
      "   0.4952964 ]\n",
      " [ 0.50063349  0.50142059  0.5048145  ...,  0.49879363  0.50175538\n",
      "   0.49921309]\n",
      " ..., \n",
      " [ 0.50020059  0.49960714  0.49948488 ...,  0.50139802  0.5001875\n",
      "   0.49577761]\n",
      " [ 0.49928757  0.49574045  0.50340919 ...,  0.49856928  0.50252313\n",
      "   0.49339625]\n",
      " [ 0.50066813  0.5003726   0.49998127 ...,  0.49861328  0.49978534\n",
      "   0.49616407]]\n",
      "[[ 0.49911601  0.49700142  0.50250932 ...,  0.49907234  0.49916929\n",
      "   0.49366697]\n",
      " [ 0.50365163  0.49626932  0.50725439 ...,  0.49639104  0.50428461\n",
      "   0.49535098]\n",
      " [ 0.50063833  0.50142583  0.50481844 ...,  0.49879909  0.50176049\n",
      "   0.49921886]\n",
      " ..., \n",
      " [ 0.50020732  0.49961406  0.49949128 ...,  0.50140497  0.50019431\n",
      "   0.49578477]\n",
      " [ 0.49934768  0.49580153  0.50346777 ...,  0.49863043  0.50258354\n",
      "   0.49345819]\n",
      " [ 0.50068423  0.50038906  0.49999677 ...,  0.49862982  0.49980163\n",
      "   0.49618096]]\n",
      "[[ 0.49911601  0.49700142  0.50250932 ...,  0.49907234  0.49916929\n",
      "   0.49366697]\n",
      " [ 0.50365163  0.49626932  0.50725439 ...,  0.49639104  0.50428461\n",
      "   0.49535098]\n",
      " [ 0.50063833  0.50142583  0.50481844 ...,  0.49879909  0.50176049\n",
      "   0.49921886]\n",
      " ..., \n",
      " [ 0.50020732  0.49961406  0.49949128 ...,  0.50140497  0.50019431\n",
      "   0.49578477]\n",
      " [ 0.49934768  0.49580153  0.50346777 ...,  0.49863043  0.50258354\n",
      "   0.49345819]\n",
      " [ 0.50068423  0.50038906  0.49999677 ...,  0.49862982  0.49980163\n",
      "   0.49618096]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " ..., \n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "[[ 0.49911601  0.49700142  0.50250932 ...,  0.49907234  0.49916929\n",
      "   0.49366697]\n",
      " [ 0.50365163  0.49626932  0.50725439 ...,  0.49639104  0.50428461\n",
      "   0.49535098]\n",
      " [ 0.50063833  0.50142583  0.50481844 ...,  0.49879909  0.50176049\n",
      "   0.49921886]\n",
      " ..., \n",
      " [ 0.50020732  0.49961406  0.49949128 ...,  0.50140497  0.50019431\n",
      "   0.49578477]\n",
      " [ 0.49934768  0.49580153  0.50346777 ...,  0.49863043  0.50258354\n",
      "   0.49345819]\n",
      " [ 0.50068423  0.50038906  0.49999677 ...,  0.49862982  0.49980163\n",
      "   0.49618096]] [[0]\n",
      " [1]\n",
      " [0]\n",
      " ..., \n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "0.124942276733\n",
      "    -> loss: 0.124942\n",
      "Epoch 6\n",
      "    -> batch size: 11339\n",
      "[[ 0.50419185  0.50106193  0.50151111 ...,  0.49912561  0.49859203\n",
      "   0.49860578]\n",
      " [ 0.49792589  0.50010934  0.4996107  ...,  0.5021991   0.49879309\n",
      "   0.49733523]\n",
      " [ 0.49865895  0.50074172  0.50252725 ...,  0.50034471  0.49738144\n",
      "   0.49446084]\n",
      " ..., \n",
      " [ 0.5022579   0.49807465  0.50557804 ...,  0.50110503  0.50001278\n",
      "   0.49600738]\n",
      " [ 0.50132949  0.49947985  0.50461675 ...,  0.49977472  0.50071019\n",
      "   0.49458117]\n",
      " [ 0.49920823  0.49666232  0.50058513 ...,  0.50351523  0.49929964\n",
      "   0.49690352]]\n",
      "[[ 0.5041861   0.50105648  0.50150457 ...,  0.49912043  0.49858656\n",
      "   0.49860086]\n",
      " [ 0.49792136  0.50010502  0.49960554 ...,  0.50219491  0.4987887\n",
      "   0.49733136]\n",
      " [ 0.4986556   0.50073889  0.50252251 ...,  0.50034222  0.49737848\n",
      "   0.49445895]\n",
      " ..., \n",
      " [ 0.5022983   0.49811565  0.50561751 ...,  0.50114606  0.50005335\n",
      "   0.49604892]\n",
      " [ 0.50137101  0.49952204  0.50465721 ...,  0.49981697  0.50075191\n",
      "   0.49462401]\n",
      " [ 0.49922069  0.4966752   0.5005967  ...,  0.50352821  0.4993123\n",
      "   0.49691699]]\n",
      "[[ 0.5041861   0.50105648  0.50150457 ...,  0.49912043  0.49858656\n",
      "   0.49860086]\n",
      " [ 0.49792136  0.50010502  0.49960554 ...,  0.50219491  0.4987887\n",
      "   0.49733136]\n",
      " [ 0.4986556   0.50073889  0.50252251 ...,  0.50034222  0.49737848\n",
      "   0.49445895]\n",
      " ..., \n",
      " [ 0.5022983   0.49811565  0.50561751 ...,  0.50114606  0.50005335\n",
      "   0.49604892]\n",
      " [ 0.50137101  0.49952204  0.50465721 ...,  0.49981697  0.50075191\n",
      "   0.49462401]\n",
      " [ 0.49922069  0.4966752   0.5005967  ...,  0.50352821  0.4993123\n",
      "   0.49691699]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " ..., \n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "[[ 0.5041861   0.50105648  0.50150457 ...,  0.49912043  0.49858656\n",
      "   0.49860086]\n",
      " [ 0.49792136  0.50010502  0.49960554 ...,  0.50219491  0.4987887\n",
      "   0.49733136]\n",
      " [ 0.4986556   0.50073889  0.50252251 ...,  0.50034222  0.49737848\n",
      "   0.49445895]\n",
      " ..., \n",
      " [ 0.5022983   0.49811565  0.50561751 ...,  0.50114606  0.50005335\n",
      "   0.49604892]\n",
      " [ 0.50137101  0.49952204  0.50465721 ...,  0.49981697  0.50075191\n",
      "   0.49462401]\n",
      " [ 0.49922069  0.4966752   0.5005967  ...,  0.50352821  0.4993123\n",
      "   0.49691699]] [[0]\n",
      " [0]\n",
      " [0]\n",
      " ..., \n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "0.124932118921\n",
      "    -> loss: 0.124932\n",
      "Epoch 7\n",
      "    -> batch size: 11339\n",
      "[[ 0.50201597  0.49981988  0.50647518 ...,  0.49590883  0.50493381\n",
      "   0.49452313]\n",
      " [ 0.49931216  0.49689983  0.5031648  ...,  0.49805021  0.5026859\n",
      "   0.49399999]\n",
      " [ 0.50022885  0.49923992  0.50580422 ...,  0.49668648  0.4983528\n",
      "   0.49510111]\n",
      " ..., \n",
      " [ 0.50207972  0.50227393  0.50794034 ...,  0.4994279   0.49489257\n",
      "   0.49876797]\n",
      " [ 0.50123924  0.50085206  0.50541671 ...,  0.49745488  0.49677368\n",
      "   0.49869013]\n",
      " [ 0.50128567  0.49785135  0.50468059 ...,  0.49708441  0.50023792\n",
      "   0.49505981]]\n",
      "[[ 0.50207019  0.49987505  0.50652794 ...,  0.49596415  0.50498837\n",
      "   0.49457916]\n",
      " [ 0.49934943  0.49693784  0.50320084 ...,  0.49808833  0.50272343\n",
      "   0.4940387 ]\n",
      " [ 0.50022257  0.49923409  0.50579673 ...,  0.49668102  0.49834689\n",
      "   0.49509603]\n",
      " ..., \n",
      " [ 0.50206325  0.50225788  0.50792257 ...,  0.49941226  0.49487654\n",
      "   0.49875276]\n",
      " [ 0.50122852  0.50084174  0.50540489 ...,  0.49744494  0.49676336\n",
      "   0.49868053]\n",
      " [ 0.50133467  0.49790117  0.50472832 ...,  0.49713432  0.5002872\n",
      "   0.49511034]]\n",
      "[[ 0.50207019  0.49987505  0.50652794 ...,  0.49596415  0.50498837\n",
      "   0.49457916]\n",
      " [ 0.49934943  0.49693784  0.50320084 ...,  0.49808833  0.50272343\n",
      "   0.4940387 ]\n",
      " [ 0.50022257  0.49923409  0.50579673 ...,  0.49668102  0.49834689\n",
      "   0.49509603]\n",
      " ..., \n",
      " [ 0.50206325  0.50225788  0.50792257 ...,  0.49941226  0.49487654\n",
      "   0.49875276]\n",
      " [ 0.50122852  0.50084174  0.50540489 ...,  0.49744494  0.49676336\n",
      "   0.49868053]\n",
      " [ 0.50133467  0.49790117  0.50472832 ...,  0.49713432  0.5002872\n",
      "   0.49511034]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "[[ 0.50207019  0.49987505  0.50652794 ...,  0.49596415  0.50498837\n",
      "   0.49457916]\n",
      " [ 0.49934943  0.49693784  0.50320084 ...,  0.49808833  0.50272343\n",
      "   0.4940387 ]\n",
      " [ 0.50022257  0.49923409  0.50579673 ...,  0.49668102  0.49834689\n",
      "   0.49509603]\n",
      " ..., \n",
      " [ 0.50206325  0.50225788  0.50792257 ...,  0.49941226  0.49487654\n",
      "   0.49875276]\n",
      " [ 0.50122852  0.50084174  0.50540489 ...,  0.49744494  0.49676336\n",
      "   0.49868053]\n",
      " [ 0.50133467  0.49790117  0.50472832 ...,  0.49713432  0.5002872\n",
      "   0.49511034]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "0.124921962889\n",
      "    -> loss: 0.124922\n",
      "Epoch 8\n",
      "    -> batch size: 11339\n",
      "[[ 0.49944095  0.49915664  0.50439322 ...,  0.49874312  0.50050803\n",
      "   0.49709013]\n",
      " [ 0.49915503  0.50149563  0.5065122  ...,  0.49735211  0.49696099\n",
      "   0.50075041]\n",
      " [ 0.50298374  0.49473687  0.50335828 ...,  0.49709988  0.49877894\n",
      "   0.49605206]\n",
      " ..., \n",
      " [ 0.50424014  0.49731757  0.50172121 ...,  0.49732813  0.4993719\n",
      "   0.49883591]\n",
      " [ 0.50246539  0.49993743  0.50219231 ...,  0.5015716   0.50102453\n",
      "   0.49858489]\n",
      " [ 0.49880866  0.49760993  0.49979467 ...,  0.50251732  0.49905825\n",
      "   0.49578947]]\n",
      "[[ 0.49947897  0.49919539  0.50443005 ...,  0.49878199  0.50054635\n",
      "   0.49712958]\n",
      " [ 0.49913903  0.5014801   0.50649487 ...,  0.49733701  0.49694543\n",
      "   0.50073568]\n",
      " [ 0.503017    0.49477091  0.5033903  ...,  0.49713411  0.49881257\n",
      "   0.49608683]\n",
      " ..., \n",
      " [ 0.50426993  0.49734802  0.50175    ...,  0.49735873  0.49940203\n",
      "   0.49886698]\n",
      " [ 0.50247583  0.49994819  0.50220218 ...,  0.50158247  0.50103515\n",
      "   0.498596  ]\n",
      " [ 0.49881229  0.4976139   0.49979749 ...,  0.50252139  0.49906206\n",
      "   0.495794  ]]\n",
      "[[ 0.49947897  0.49919539  0.50443005 ...,  0.49878199  0.50054635\n",
      "   0.49712958]\n",
      " [ 0.49913903  0.5014801   0.50649487 ...,  0.49733701  0.49694543\n",
      "   0.50073568]\n",
      " [ 0.503017    0.49477091  0.5033903  ...,  0.49713411  0.49881257\n",
      "   0.49608683]\n",
      " ..., \n",
      " [ 0.50426993  0.49734802  0.50175    ...,  0.49735873  0.49940203\n",
      "   0.49886698]\n",
      " [ 0.50247583  0.49994819  0.50220218 ...,  0.50158247  0.50103515\n",
      "   0.498596  ]\n",
      " [ 0.49881229  0.4976139   0.49979749 ...,  0.50252139  0.49906206\n",
      "   0.495794  ]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "[[ 0.49947897  0.49919539  0.50443005 ...,  0.49878199  0.50054635\n",
      "   0.49712958]\n",
      " [ 0.49913903  0.5014801   0.50649487 ...,  0.49733701  0.49694543\n",
      "   0.50073568]\n",
      " [ 0.503017    0.49477091  0.5033903  ...,  0.49713411  0.49881257\n",
      "   0.49608683]\n",
      " ..., \n",
      " [ 0.50426993  0.49734802  0.50175    ...,  0.49735873  0.49940203\n",
      "   0.49886698]\n",
      " [ 0.50247583  0.49994819  0.50220218 ...,  0.50158247  0.50103515\n",
      "   0.498596  ]\n",
      " [ 0.49881229  0.4976139   0.49979749 ...,  0.50252139  0.49906206\n",
      "   0.495794  ]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.124911808638\n",
      "    -> loss: 0.124912\n",
      "Epoch 9\n",
      "    -> batch size: 11339\n",
      "[[ 0.50412606  0.4970016   0.50327795 ...,  0.49676544  0.50416365\n",
      "   0.4921359 ]\n",
      " [ 0.50252131  0.49602143  0.50389967 ...,  0.49725481  0.50185876\n",
      "   0.49459778]\n",
      " [ 0.5020911   0.5000679   0.50428487 ...,  0.49845328  0.49958154\n",
      "   0.49772015]\n",
      " ..., \n",
      " [ 0.49823438  0.49748876  0.50288363 ...,  0.50127375  0.50022516\n",
      "   0.50097903]\n",
      " [ 0.50071558  0.49936253  0.50831902 ...,  0.49379617  0.49513207\n",
      "   0.49478008]\n",
      " [ 0.50420843  0.49985253  0.50165015 ...,  0.49859743  0.50419183\n",
      "   0.49361998]]\n",
      "[[ 0.50417632  0.49705276  0.5033269  ...,  0.49681673  0.50421424\n",
      "   0.49218783]\n",
      " [ 0.50257102  0.49607203  0.50394806 ...,  0.49730553  0.50190883\n",
      "   0.49464919]\n",
      " [ 0.502088    0.50006533  0.50428051 ...,  0.49845107  0.49957883\n",
      "   0.49771835]\n",
      " ..., \n",
      " [ 0.49824334  0.49749796  0.50289213 ...,  0.50128297  0.50023422\n",
      "   0.50098855]\n",
      " [ 0.50070639  0.49935396  0.50830825 ...,  0.49378809  0.49512337\n",
      "   0.49477245]\n",
      " [ 0.50425033  0.49989505  0.50169112 ...,  0.49863999  0.50423389\n",
      "   0.49366307]]\n",
      "[[ 0.50417632  0.49705276  0.5033269  ...,  0.49681673  0.50421424\n",
      "   0.49218783]\n",
      " [ 0.50257102  0.49607203  0.50394806 ...,  0.49730553  0.50190883\n",
      "   0.49464919]\n",
      " [ 0.502088    0.50006533  0.50428051 ...,  0.49845107  0.49957883\n",
      "   0.49771835]\n",
      " ..., \n",
      " [ 0.49824334  0.49749796  0.50289213 ...,  0.50128297  0.50023422\n",
      "   0.50098855]\n",
      " [ 0.50070639  0.49935396  0.50830825 ...,  0.49378809  0.49512337\n",
      "   0.49477245]\n",
      " [ 0.50425033  0.49989505  0.50169112 ...,  0.49863999  0.50423389\n",
      "   0.49366307]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "[[ 0.50417632  0.49705276  0.5033269  ...,  0.49681673  0.50421424\n",
      "   0.49218783]\n",
      " [ 0.50257102  0.49607203  0.50394806 ...,  0.49730553  0.50190883\n",
      "   0.49464919]\n",
      " [ 0.502088    0.50006533  0.50428051 ...,  0.49845107  0.49957883\n",
      "   0.49771835]\n",
      " ..., \n",
      " [ 0.49824334  0.49749796  0.50289213 ...,  0.50128297  0.50023422\n",
      "   0.50098855]\n",
      " [ 0.50070639  0.49935396  0.50830825 ...,  0.49378809  0.49512337\n",
      "   0.49477245]\n",
      " [ 0.50425033  0.49989505  0.50169112 ...,  0.49863999  0.50423389\n",
      "   0.49366307]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "0.124901656166\n",
      "    -> loss: 0.124902\n",
      "Epoch 10\n",
      "    -> batch size: 11339\n",
      "[[ 0.50060117  0.4984643   0.50687147 ...,  0.49708821  0.50213078\n",
      "   0.49585669]\n",
      " [ 0.49949229  0.49839823  0.50130595 ...,  0.50282001  0.50164668\n",
      "   0.49466319]\n",
      " [ 0.50369469  0.49758892  0.50326578 ...,  0.499522    0.49984788\n",
      "   0.4969807 ]\n",
      " ..., \n",
      " [ 0.49994761  0.50423194  0.50271178 ...,  0.50075079  0.50039704\n",
      "   0.49771084]\n",
      " [ 0.50107227  0.50111288  0.50531645 ...,  0.49825891  0.49776205\n",
      "   0.49821206]\n",
      " [ 0.503317    0.5003071   0.50467844 ...,  0.49921257  0.50077362\n",
      "   0.49624147]]\n",
      "[[ 0.50066203  0.49852627  0.50693056 ...,  0.49715034  0.50219206\n",
      "   0.49591967]\n",
      " [ 0.49950674  0.49841298  0.5013199  ...,  0.50283478  0.50166123\n",
      "   0.49467828]\n",
      " [ 0.50373891  0.49763392  0.50330887 ...,  0.4995671   0.49989243\n",
      "   0.4970264 ]\n",
      " ..., \n",
      " [ 0.49994326  0.50422796  0.50270643 ...,  0.5007471   0.50039297\n",
      "   0.49770756]\n",
      " [ 0.50107465  0.50111588  0.50531743 ...,  0.49826228  0.4977649\n",
      "   0.49821594]\n",
      " [ 0.50335441  0.50034525  0.50471461 ...,  0.49925084  0.50081131\n",
      "   0.49628036]]\n",
      "[[ 0.50066203  0.49852627  0.50693056 ...,  0.49715034  0.50219206\n",
      "   0.49591967]\n",
      " [ 0.49950674  0.49841298  0.5013199  ...,  0.50283478  0.50166123\n",
      "   0.49467828]\n",
      " [ 0.50373891  0.49763392  0.50330887 ...,  0.4995671   0.49989243\n",
      "   0.4970264 ]\n",
      " ..., \n",
      " [ 0.49994326  0.50422796  0.50270643 ...,  0.5007471   0.50039297\n",
      "   0.49770756]\n",
      " [ 0.50107465  0.50111588  0.50531743 ...,  0.49826228  0.4977649\n",
      "   0.49821594]\n",
      " [ 0.50335441  0.50034525  0.50471461 ...,  0.49925084  0.50081131\n",
      "   0.49628036]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "[[ 0.50066203  0.49852627  0.50693056 ...,  0.49715034  0.50219206\n",
      "   0.49591967]\n",
      " [ 0.49950674  0.49841298  0.5013199  ...,  0.50283478  0.50166123\n",
      "   0.49467828]\n",
      " [ 0.50373891  0.49763392  0.50330887 ...,  0.4995671   0.49989243\n",
      "   0.4970264 ]\n",
      " ..., \n",
      " [ 0.49994326  0.50422796  0.50270643 ...,  0.5007471   0.50039297\n",
      "   0.49770756]\n",
      " [ 0.50107465  0.50111588  0.50531743 ...,  0.49826228  0.4977649\n",
      "   0.49821594]\n",
      " [ 0.50335441  0.50034525  0.50471461 ...,  0.49925084  0.50081131\n",
      "   0.49628036]] [[1]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "0.124891505473\n",
      "    -> loss: 0.124892\n"
     ]
    }
   ],
   "source": [
    "##### Ćwiczenie 4\n",
    "\n",
    "model = Network(MSE(), GD(0.1))\n",
    "model.add(Dense(28*28, 28*28))\n",
    "model.add(Sigmoid())\n",
    "model.fit(X_train, y_train, 10, batch_size=X_train.shape[0], print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ćwiczenie 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ćwiczenie 7\n",
    "model = Network(Crossentropy(), GD(0.1))\n",
    "model.add(Dense(28*28, 28*28))\n",
    "model.add(Sigmoid())\n",
    "model.fit(X_train, y_train, 10, batch_size=X_train.shape[0], print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "(1, 4)\n",
      "[[-0.05424164  0.16972465 -0.41749995  0.39709858]]\n",
      "out_grad_4:\n",
      "(1, 4)\n",
      "[[-0.2019965  -0.23769518 -0.49487045  0.04320252]]\n",
      "out_grad_3:\n",
      "(1, 3)\n",
      "[[-0.02440363  0.13637368  0.47820413]]\n",
      "Testing d1...\n",
      "d1.forward_pass(inp):\n",
      "(1, 3)\n",
      "[[-0.02707048 -0.20288487 -0.03403094]]\n",
      "d1.backward_pass(inp, out_grad_3):\n",
      "(1, 4)\n",
      "[[-0.01685764  0.21647157  0.04530229  0.03971692]]\n",
      "(15,)\n",
      "[-0.02440363  0.13637368  0.47820413  0.00132369 -0.00739713 -0.02593858\n",
      " -0.0041419   0.02314597  0.08116303  0.01018852 -0.056936   -0.1996502\n",
      " -0.00969065  0.05415379  0.18989418]\n",
      "Testing d2...\n",
      "d2.forward_pass(inp):\n",
      "(1, 3)\n",
      "[[ 0.  0.  0.]]\n",
      "d2.backward_pass(inp, out_grad_3):\n",
      "(1, 4)\n",
      "[[ 0.  0.  0.  0.]]\n",
      "(15,)\n",
      "[-0.02440363  0.13637368  0.47820413  0.00132369 -0.00739713 -0.02593858\n",
      " -0.0041419   0.02314597  0.08116303  0.01018852 -0.056936   -0.1996502\n",
      " -0.00969065  0.05415379  0.18989418]\n",
      "Testing r...\n",
      "r.forward_pass(inp):\n",
      "(1, 4)\n",
      "[[ 0.          0.16972465  0.          0.39709858]]\n",
      "r.backward_pass(inp, out_grad_4):\n",
      "(1, 4)\n",
      "[[-0.         -0.23769518 -0.          0.04320252]]\n",
      "None\n",
      "Testing s...\n",
      "s.forward_pass(inp):\n",
      "(1, 4)\n",
      "[[ 0.48644291  0.5423296   0.39711514  0.59799036]]\n",
      "s.backward_pass(inp, out_grad_4):\n",
      "(1, 4)\n",
      "[[-0.050462   -0.05899789 -0.11847926  0.01038579]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "d1 = Dense(input_size=4, output_size=3, init=\"gaussian\")\n",
    "d2 = Dense(input_size=4, output_size=3, init=\"zeros\")\n",
    "r = ReLU()\n",
    "s = Sigmoid()\n",
    "inp = np.random.random(4).reshape((1,-1)) - 0.5\n",
    "out_grad_4 = np.random.random(4).reshape((1,-1)) - 0.5\n",
    "out_grad_3 = np.random.random(3).reshape((1,-1)) - 0.5\n",
    "\n",
    "print \"inp:\"\n",
    "print inp.shape\n",
    "print inp\n",
    "print \"out_grad_4:\"\n",
    "print out_grad_4.shape\n",
    "print out_grad_4\n",
    "print \"out_grad_3:\"\n",
    "print out_grad_3.shape\n",
    "print out_grad_3\n",
    "\n",
    "print \"Testing d1...\"\n",
    "print \"d1.forward_pass(inp):\"\n",
    "t = d1.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"d1.backward_pass(inp, out_grad_3):\"\n",
    "t = d1.backward_pass(inp, out_grad_3)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1].shape\n",
    "print t[1]\n",
    "\n",
    "print \"Testing d2...\"\n",
    "print \"d2.forward_pass(inp):\"\n",
    "t = d2.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"d2.backward_pass(inp, out_grad_3):\"\n",
    "t = d2.backward_pass(inp, out_grad_3)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1].shape\n",
    "print t[1]\n",
    "\n",
    "print \"Testing r...\"\n",
    "print \"r.forward_pass(inp):\"\n",
    "t = r.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"r.backward_pass(inp, out_grad_4):\"\n",
    "t = r.backward_pass(inp, out_grad_4)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1]\n",
    "\n",
    "print \"Testing s...\"\n",
    "print \"s.forward_pass(inp):\"\n",
    "t = s.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"s.backward_pass(inp, out_grad_4):\"\n",
    "t = s.backward_pass(inp, out_grad_4)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    inp:\n",
    "    (1, 4)\n",
    "    [[-0.05424164  0.16972465 -0.41749995  0.39709858]]\n",
    "    out_grad_4:\n",
    "    (1, 4)\n",
    "    [[-0.2019965  -0.23769518 -0.49487045  0.04320252]]\n",
    "    out_grad_3:\n",
    "    (1, 3)\n",
    "    [[-0.02440363  0.13637368  0.47820413]]\n",
    "    Testing d1...\n",
    "    d1.forward_pass(inp):\n",
    "    (1, 3)\n",
    "    [[-0.02707048 -0.20288487 -0.03403094]]\n",
    "    d1.backward_pass(inp, out_grad_3):\n",
    "    (1, 4)\n",
    "    [[-0.01685764  0.21647157  0.04530229  0.03971692]]\n",
    "    (15,)\n",
    "    [-0.02440363  0.13637368  0.47820413  0.00132369 -0.00739713 -0.02593858\n",
    "     -0.0041419   0.02314597  0.08116303  0.01018852 -0.056936   -0.1996502\n",
    "     -0.00969065  0.05415379  0.18989418]\n",
    "    Testing d2...\n",
    "    d2.forward_pass(inp):\n",
    "    (1, 3)\n",
    "    [[ 0.  0.  0.]]\n",
    "    d2.backward_pass(inp, out_grad_3):\n",
    "    (1, 4)\n",
    "    [[ 0.  0.  0.  0.]]\n",
    "    (15,)\n",
    "    [-0.02440363  0.13637368  0.47820413  0.00132369 -0.00739713 -0.02593858\n",
    "     -0.0041419   0.02314597  0.08116303  0.01018852 -0.056936   -0.1996502\n",
    "     -0.00969065  0.05415379  0.18989418]\n",
    "    Testing r...\n",
    "    r.forward_pass(inp):\n",
    "    (1, 4)\n",
    "    [[ 0.          0.16972465  0.          0.39709858]]\n",
    "    r.backward_pass(inp, out_grad_4):\n",
    "    (1, 4)\n",
    "    [[ 0.         -0.23769518  0.          0.04320252]]\n",
    "    None\n",
    "    Testing s...\n",
    "    s.forward_pass(inp):\n",
    "    (1, 4)\n",
    "    [[ 0.48644291  0.5423296   0.39711514  0.59799036]]\n",
    "    s.backward_pass(inp, out_grad_4):\n",
    "    (1, 4)\n",
    "    [[-0.050462   -0.05899789 -0.11847926  0.01038579]]\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "[[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
      " [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
      "inps[0]:\n",
      "[[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
      " [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
      "inps[1]:\n",
      "[[ 0.06231216 -0.25663962 -0.11549112]\n",
      " [ 0.03836232 -0.10585852 -0.03015728]]\n",
      "inps[2]:\n",
      "[[ 0.06231216  0.          0.        ]\n",
      " [ 0.03836232  0.          0.        ]]\n",
      "inps[3]:\n",
      "[[-0.04493685  0.4840028  -0.15711608 -0.22696511]\n",
      " [-0.0396103   0.48391873 -0.15615311 -0.24031931]]\n",
      "inps[4]:\n",
      "[[ 0.          0.4840028   0.          0.        ]\n",
      " [ 0.          0.48391873  0.          0.        ]]\n",
      "inps[5]:\n",
      "[[ 0.29948146]\n",
      " [ 0.29947621]]\n",
      "out:\n",
      "[[ 0.57431575]\n",
      " [ 0.57431447]]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "np.random.seed(43)\n",
    "n = Network(loss=MSE(), optimizer=GD(learning_rate=0.001), metrics=[])\n",
    "n.add(Dense(input_size=4, output_size=3, init=\"gaussian\"))#0\n",
    "n.add(ReLU())#1\n",
    "n.add(Dense(input_size=3, output_size=4, init=\"gaussian\"))#2\n",
    "n.add(ReLU())#3\n",
    "n.add(Dense(input_size=4, output_size=1, init=\"gaussian\"))#4\n",
    "n.add(Sigmoid())#5\n",
    "inp = np.random.random((2,4)) - 0.5\n",
    "inps, out = n._forward_pass(inp)\n",
    "print \"inp:\"\n",
    "print inp\n",
    "for i, inp in enumerate(inps):\n",
    "    print \"inps[\" + str(i) + \"]:\"\n",
    "    print inp\n",
    "print \"out:\"\n",
    "print out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    inp:\n",
    "    [[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
    "     [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
    "    inps[0]:\n",
    "    [[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
    "     [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
    "    inps[1]:\n",
    "    [[ 0.06231216 -0.25663962 -0.11549112]\n",
    "     [ 0.03836232 -0.10585852 -0.03015728]]\n",
    "    inps[2]:\n",
    "    [[ 0.06231216  0.          0.        ]\n",
    "     [ 0.03836232  0.          0.        ]]\n",
    "    inps[3]:\n",
    "    [[-0.04493685  0.4840028  -0.15711608 -0.22696511]\n",
    "     [-0.0396103   0.48391873 -0.15615311 -0.24031931]]\n",
    "    inps[4]:\n",
    "    [[ 0.          0.4840028   0.          0.        ]\n",
    "     [ 0.          0.48391873  0.          0.        ]]\n",
    "    inps[5]:\n",
    "    [[ 0.29948146]\n",
    "     [ 0.29947621]]\n",
    "    out:\n",
    "    [[ 0.57431575]\n",
    "     [ 0.57431447]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "[[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
      " [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
      "target:\n",
      "[[ 0.06066317]\n",
      " [ 0.17014516]]\n",
      "inps[0]:\n",
      "[[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
      " [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
      "inps[1]:\n",
      "[[ 0.06231216 -0.25663962 -0.11549112]\n",
      " [ 0.03836232 -0.10585852 -0.03015728]]\n",
      "inps[2]:\n",
      "[[ 0.06231216  0.          0.        ]\n",
      " [ 0.03836232  0.          0.        ]]\n",
      "inps[3]:\n",
      "[[-0.04493685  0.4840028  -0.15711608 -0.22696511]\n",
      " [-0.0396103   0.48391873 -0.15615311 -0.24031931]]\n",
      "inps[4]:\n",
      "[[ 0.          0.4840028   0.          0.        ]\n",
      " [ 0.          0.48391873  0.          0.        ]]\n",
      "inps[5]:\n",
      "[[ 0.29948146]\n",
      " [ 0.29947621]]\n",
      "out:\n",
      "[[ 0.57431575]\n",
      " [ 0.57431447]]\n",
      "grad:\n",
      "[[ 0.25682629]\n",
      " [ 0.20208465]]\n",
      "layer_grads[0]:\n",
      "[  2.45711515e-05   0.00000000e+00   0.00000000e+00   2.67516826e-06\n",
      "   0.00000000e+00   0.00000000e+00   9.94708194e-07   0.00000000e+00\n",
      "   0.00000000e+00   4.30677425e-06   0.00000000e+00   0.00000000e+00\n",
      "  -9.52974216e-07   0.00000000e+00   0.00000000e+00]\n",
      "layer_grads[1]:\n",
      "None\n",
      "layer_grads[2]:\n",
      "[ 0.          0.00700006  0.          0.          0.          0.00036236\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "layer_grads[3]:\n",
      "None\n",
      "layer_grads[4]:\n",
      "[ 0.11219329  0.          0.05429771  0.          0.        ]\n",
      "layer_grads[5]:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "n = Network(loss=MSE(), optimizer=GD(learning_rate=0.001), metrics=[])\n",
    "n.add(Dense(input_size=4, output_size=3, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=3, output_size=4, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=4, output_size=1, init=\"gaussian\"))\n",
    "n.add(Sigmoid())\n",
    "\n",
    "inp = np.random.random((2,4)) - 0.5\n",
    "target = inp[:,0:1]\n",
    "inps, out = n._forward_pass(inp)\n",
    "grad = n.loss.backward_pass(out, target)\n",
    "layer_grads = n._backward_pass(inps, grad)\n",
    "\n",
    "print \"inp:\"\n",
    "print inp\n",
    "print \"target:\"\n",
    "print target\n",
    "for i, inp in enumerate(inps):\n",
    "    print \"inps[\" + str(i) + \"]:\"\n",
    "    print inp\n",
    "print \"out:\"\n",
    "print out\n",
    "print \"grad:\"\n",
    "print grad\n",
    "for i, grad in enumerate(layer_grads):\n",
    "    print \"layer_grads[\" + str(i) + \"]:\"\n",
    "    print grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    inp:\n",
    "    [[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
    "     [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
    "    target:\n",
    "    [[ 0.06066317]\n",
    "     [ 0.17014516]]\n",
    "    inps[0]:\n",
    "    [[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
    "     [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
    "    inps[1]:\n",
    "    [[ 0.06231216 -0.25663962 -0.11549112]\n",
    "     [ 0.03836232 -0.10585852 -0.03015728]]\n",
    "    inps[2]:\n",
    "    [[ 0.06231216  0.          0.        ]\n",
    "     [ 0.03836232  0.          0.        ]]\n",
    "    inps[3]:\n",
    "    [[-0.04493685  0.4840028  -0.15711608 -0.22696511]\n",
    "     [-0.0396103   0.48391873 -0.15615311 -0.24031931]]\n",
    "    inps[4]:\n",
    "    [[ 0.          0.4840028   0.          0.        ]\n",
    "     [ 0.          0.48391873  0.          0.        ]]\n",
    "    inps[5]:\n",
    "    [[ 0.29948146]\n",
    "     [ 0.29947621]]\n",
    "    out:\n",
    "    [[ 0.57431575]\n",
    "     [ 0.57431447]]\n",
    "    grad:\n",
    "    [[ 0.25682629]\n",
    "     [ 0.20208465]]\n",
    "    layer_grads[0]:\n",
    "    [  2.45711515e-05   0.00000000e+00   0.00000000e+00   2.67516826e-06\n",
    "       0.00000000e+00   0.00000000e+00   9.94708194e-07   0.00000000e+00\n",
    "       0.00000000e+00   4.30677425e-06   0.00000000e+00   0.00000000e+00\n",
    "      -9.52974216e-07   0.00000000e+00   0.00000000e+00]\n",
    "    layer_grads[1]:\n",
    "    None\n",
    "    layer_grads[2]:\n",
    "    [ 0.          0.00700006  0.          0.          0.          0.00036236\n",
    "      0.          0.          0.          0.          0.          0.          0.\n",
    "      0.          0.          0.        ]\n",
    "    layer_grads[3]:\n",
    "    None\n",
    "    layer_grads[4]:\n",
    "    [ 0.11219329  0.          0.05429771  0.          0.        ]\n",
    "    layer_grads[5]:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(43)\n",
    "ce = Crossentropy()\n",
    "y = np.random.random(10)\n",
    "t = np.random.random(10)\n",
    "\n",
    "print \"y:\"\n",
    "print y\n",
    "print \"t:\"\n",
    "print t\n",
    "print \"ce.forward_pass(y,t):\"\n",
    "print ce.forward_pass(y,t)\n",
    "print \"ce.backward_pass(y,t):\"\n",
    "print ce.backward_pass(y,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    y:\n",
    "    [ 0.11505457  0.60906654  0.13339096  0.24058962  0.32713906  0.85913749\n",
    "      0.66609021  0.54116221  0.02901382  0.7337483 ]\n",
    "    t:\n",
    "    [ 0.39495002  0.80204712  0.25442113  0.05688494  0.86664864  0.221029\n",
    "      0.40498945  0.31609647  0.0766627   0.84322469]\n",
    "    ce.forward_pass(y,t):\n",
    "    0.736415962327\n",
    "    ce.backward_pass(y,t):\n",
    "    [-0.27490047 -0.08104869 -0.10469935  0.10054647 -0.24509895  0.5272741\n",
    "      0.11739401  0.0906406  -0.16913545 -0.05603779]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad1:\n",
      "[ 0.11505457  0.60906654  0.13339096  0.24058962  0.32713906]\n",
      "grad2:\n",
      "[ 0.85913749  0.66609021  0.54116221  0.02901382  0.7337483 ]\n",
      "grad3:\n",
      "[ 0.39495002  0.80204712  0.25442113  0.05688494  0.86664864]\n",
      "opt.calculate_deltas(grad1):\n",
      "[-0.02919466 -0.0529381  -0.0209018  -0.01117572 -0.04443381]\n",
      "opt.calculate_deltas(grad2):\n",
      "[-0.04608546 -0.06573052 -0.03151603 -0.01164424 -0.05866444]\n",
      "opt.calculate_deltas(grad3):\n",
      "[-0.05352361 -0.08111416 -0.03628929 -0.0126655  -0.07541077]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "opt = Momentum(alpha=0.02, beta=0.99)\n",
    "grad1 = np.random.random(5)\n",
    "grad2 = np.random.random(5)\n",
    "grad3 = np.random.random(5)\n",
    "opt.calculate_deltas(grad1)\n",
    "opt.calculate_deltas(grad2)\n",
    "opt.calculate_deltas(grad3)\n",
    "\n",
    "print \"grad1:\"\n",
    "print grad1\n",
    "print \"grad2:\"\n",
    "print grad2\n",
    "print \"grad3:\"\n",
    "print grad3\n",
    "\n",
    "print \"opt.calculate_deltas(grad1):\"\n",
    "print opt.calculate_deltas(grad1)\n",
    "print \"opt.calculate_deltas(grad2):\"\n",
    "print opt.calculate_deltas(grad2)\n",
    "print \"opt.calculate_deltas(grad3):\"\n",
    "print opt.calculate_deltas(grad3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    grad1:\n",
    "    [ 0.11505457  0.60906654  0.13339096  0.24058962  0.32713906]\n",
    "    grad2:\n",
    "    [ 0.85913749  0.66609021  0.54116221  0.02901382  0.7337483 ]\n",
    "    grad3:\n",
    "    [ 0.39495002  0.80204712  0.25442113  0.05688494  0.86664864]\n",
    "    opt.calculate_deltas(grad1):\n",
    "    [-0.02919466 -0.0529381  -0.0209018  -0.01117572 -0.04443381]\n",
    "    opt.calculate_deltas(grad2):\n",
    "    [-0.04608546 -0.06573052 -0.03151603 -0.01164424 -0.05866444]\n",
    "    opt.calculate_deltas(grad3):\n",
    "    [-0.05352361 -0.08111416 -0.03628929 -0.0126655  -0.07541077]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
