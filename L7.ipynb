{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L7: Klasyfikacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na dzisiejszych ćwiczeniach zajmiemy się klasyfikacją na przykładzie problemu przewidywania raka piersi. Przypomnimy sobie pojęcia omawiane na zajęciach: model evaluation, posterior/likelihood. \n",
    "\n",
    "Będziemy chcieli znaleźć optymalny model dla niestandardowych sytuacji klasyfikacyjnych (kiedy nie tylko accuracy jest najważniejsze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metryki\n",
    "\n",
    "Przypomnijmy, że zadaniem uczenia maszynowego jest znalezienie modelu, który minimalizuje loss na zbiorze testowym (notebooki L5 oraz L6). Tutaj zajmujemy się definicjami metryk, z których każda także może być funkcją kosztu.\n",
    "\n",
    "<img width=300 src=\"wyklady2017/mum_figures/precision_recall.png\">\n",
    "\n",
    "Niech $y$ to prawdziwa klasa, a $\\hat{y}$ to predykcja. Najpopularniejsze metryki dla klasyfikatorów binarnych:\n",
    "\n",
    "* Accuracy\n",
    "\n",
    "$ \\frac{TP + TN}{TN + FN + TP + FP} = p(\\hat{y} = y | x) $\n",
    "\n",
    "* Precision \n",
    "\n",
    "$ \\frac{TP}{TP + FP} = p(y=1| \\hat{y} =1) $\n",
    "\n",
    "* Recall \n",
    "\n",
    "$ \\frac{TP}{TP + FN} = p(\\hat{y}=1| y =1) $\n",
    "\n",
    "## Bardziej zaawansowane metryki\n",
    "\n",
    "Kolejne metryki (dla klasyfikatorów binarnych) będą oparte o confusion matrix:\n",
    "\n",
    "<img src=\"figures/L7/confusion_matrix.png\">\n",
    "\n",
    "TODO: Wpisać F1 i MCC. Jakie są różnice?\n",
    "\n",
    "## Funkcja kosztu\n",
    "\n",
    "Na podstawie confusion matrix możemy definiować funkcję kosztu. Ile płacimy za FN? W przypadku klasyfikacji raka, dużo \"tańsze\" jest skierowanie pacjenta na dodatkowe badania niż postawienie fałszywej negatywnej diagnozy!\n",
    "\n",
    "<img width=400 src=\"figures/L7/cost_mat.png\">\n",
    "\n",
    "Oczywiście zazwyczaj $C_{TP}$ oraz $C_{TN}$ jest 0. Funkcja kosztu 0-1 (albo accuracy) odtwarza $C_{FN} = C_{FP}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja wrażliwa na koszt\n",
    "\n",
    "Ref: http://web.cs.iastate.edu/~honavar/elkan.pdf\n",
    "\n",
    "Czasami jesteśmy bardziej zainteresowani w precision lub recall. Są to problemy ``cost-sensitive``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>0.372583</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id   diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n",
       "count  5.690000e+02  569.000000   569.000000    569.000000      569.000000   \n",
       "mean   3.037183e+07    0.372583    14.127292     19.289649       91.969033   \n",
       "std    1.250206e+08    0.483918     3.524049      4.301036       24.298981   \n",
       "min    8.670000e+03    0.000000     6.981000      9.710000       43.790000   \n",
       "25%    8.692180e+05    0.000000    11.700000     16.170000       75.170000   \n",
       "50%    9.060240e+05    0.000000    13.370000     18.840000       86.240000   \n",
       "75%    8.813129e+06    1.000000    15.780000     21.800000      104.100000   \n",
       "max    9.113205e+08    1.000000    28.110000     39.280000      188.500000   \n",
       "\n",
       "         area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "count   569.000000       569.000000        569.000000      569.000000   \n",
       "mean    654.889104         0.096360          0.104341        0.088799   \n",
       "std     351.914129         0.014064          0.052813        0.079720   \n",
       "min     143.500000         0.052630          0.019380        0.000000   \n",
       "25%     420.300000         0.086370          0.064920        0.029560   \n",
       "50%     551.100000         0.095870          0.092630        0.061540   \n",
       "75%     782.700000         0.105300          0.130400        0.130700   \n",
       "max    2501.000000         0.163400          0.345400        0.426800   \n",
       "\n",
       "       concave points_mean           ...             radius_worst  \\\n",
       "count           569.000000           ...               569.000000   \n",
       "mean              0.048919           ...                16.269190   \n",
       "std               0.038803           ...                 4.833242   \n",
       "min               0.000000           ...                 7.930000   \n",
       "25%               0.020310           ...                13.010000   \n",
       "50%               0.033500           ...                14.970000   \n",
       "75%               0.074000           ...                18.790000   \n",
       "max               0.201200           ...                36.040000   \n",
       "\n",
       "       texture_worst  perimeter_worst   area_worst  smoothness_worst  \\\n",
       "count     569.000000       569.000000   569.000000        569.000000   \n",
       "mean       25.677223       107.261213   880.583128          0.132369   \n",
       "std         6.146258        33.602542   569.356993          0.022832   \n",
       "min        12.020000        50.410000   185.200000          0.071170   \n",
       "25%        21.080000        84.110000   515.300000          0.116600   \n",
       "50%        25.410000        97.660000   686.500000          0.131300   \n",
       "75%        29.720000       125.400000  1084.000000          0.146000   \n",
       "max        49.540000       251.200000  4254.000000          0.222600   \n",
       "\n",
       "       compactness_worst  concavity_worst  concave points_worst  \\\n",
       "count         569.000000       569.000000            569.000000   \n",
       "mean            0.254265         0.272188              0.114606   \n",
       "std             0.157336         0.208624              0.065732   \n",
       "min             0.027290         0.000000              0.000000   \n",
       "25%             0.147200         0.114500              0.064930   \n",
       "50%             0.211900         0.226700              0.099930   \n",
       "75%             0.339100         0.382900              0.161400   \n",
       "max             1.058000         1.252000              0.291000   \n",
       "\n",
       "       symmetry_worst  fractal_dimension_worst  \n",
       "count      569.000000               569.000000  \n",
       "mean         0.290076                 0.083946  \n",
       "std          0.061867                 0.018061  \n",
       "min          0.156500                 0.055040  \n",
       "25%          0.250400                 0.071460  \n",
       "50%          0.282200                 0.080040  \n",
       "75%          0.317900                 0.092080  \n",
       "max          0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/kudkudak/Downloads/data.csv\")\n",
    "data.drop(\"Unnamed: 32\",axis=1,inplace=True)\n",
    "data['diagnosis']=data['diagnosis'].map({'M':1,'B':0})\n",
    "data.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11a115950>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAECCAYAAAD9z2x7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noxB7I0QXMLIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119ed2910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['diagnosis'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wybieramy cechy\n",
    "prediction_var = ['texture_mean','perimeter_mean','smoothness_mean','compactness_mean','symmetry_mean']\n",
    "train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test\n",
    "train_X = train[prediction_var][0:100]\n",
    "train_y=train.diagnosis[0:100]\n",
    "test_X= test[prediction_var] \n",
    "test_y =test.diagnosis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.853801169591\n",
      "0.645161290323\n",
      "0.93023255814\n"
     ]
    }
   ],
   "source": [
    "# Regresja logistyczna\n",
    "model = LogisticRegression(C=1)\n",
    "model.fit(train_X,train_y)\n",
    "pred = model.predict(test_X)\n",
    "print metrics.accuracy_score(pred, test_y)\n",
    "print metrics.precision_score(pred, test_y)\n",
    "print metrics.recall_score(pred, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 [3 pkt]\n",
    "\n",
    "1. Powyższy model ma istotnie wyższy recall niż precision. \n",
    "\n",
    "Zdefiniujmy jako model probabilistyczny model, który zwraca p($\\hat{y}$ | y). Obiekt LogisticRegression zwraca tą wartość funkcją ``predict_proba``\n",
    "\n",
    "1. Każdy model probabilistyczny można użyć do stworzenia klasyfikora, która może mieć precision 100% lub recall 100% trywialnie, jak? \n",
    "\n",
    "2. Krzywa precision/recall jest obliczana licząc precision oraz recall modelu probabilistycznego dla różnych wartości precision. Zarysuj wykres precision/recall dla modelu powyżej.\n",
    "\n",
    "Powinno wyjść:\n",
    "\n",
    "<img src=\"figures/L7/prec_recall.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2 [3 pkt]\n",
    "\n",
    "Założmy, że $C_{FP}$ = 1 i $C_{FN}$ = 10, co odpowiada sytuacji w której nie przejmujemy się postawieniem fałszywej pozytywnej diagnozy.\n",
    "\n",
    "Według http://web.cs.iastate.edu/~honavar/elkan.pdf wystarczy w takim wypadku dodać wagi przykładom odpowiedniom wage.\n",
    "\n",
    "* Przetestuj pare wag klasy negatywnej przez podanie argumentu class_weight do LogisticRegression. Dla każdej wartości wagi narysuj dokładność (accuracy) oraz wynik metryki FN_aversive. Powinno wyjść:\n",
    "\n",
    "<img src=\"figures/L7/fn_aversive.png\">\n",
    "\n",
    "* Równoważnym sposobem tworzenia \"cost-sensitive\" klasyfikatora z modelu probabilistycznego jest zmiana progu (patrz Zadanie 1). Znajdź taki próg, aby wynik klasyfikatora z tym progiem był równoważny argumentowi class_weight.\n",
    "\n",
    "Podpowiedź: Jeśli 2 sprawia problem, przejrzyj załączoną publikację"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FN_aversive(y_true, y_pred):\n",
    "    FN = sum((y_true == 1) * (y_pred != y_true))\n",
    "    FP = sum((y_true == 0) * (y_pred != y_true))\n",
    "    return 10 * FN + FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes vs Regresja Logistyczna \n",
    "\n",
    "Ref: https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(train_X,train_y)\n",
    "pred = model.predict(test_X)\n",
    "print metrics.accuracy_score(pred, test_y)\n",
    "print metrics.precision_score(pred, test_y)\n",
    "print metrics.recall_score(pred, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyprowadzenie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Różnice\n",
    "\n",
    "* Naive Bayes zakłada niezależność cech (brak korekty liniowych zależności)\n",
    "\n",
    "* Naive Bayes to *model generatywny*, a *regresja logistyczna* to model dyskryminatywny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3 [3 pkt]\n",
    "\n",
    "Zilustruj na przykładzie problem \"double counting\" w modelu Naive Bayes na przykładzie klasyfikatora SPAMu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
