{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "<big><big><big><big><big><big><big><big><big>Sieci neuronowe</big></big></big></big></big></big></big></big></big>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "<id=tocheading><big><big><big><big><big>Spis treści</big></big></big></big></big>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set(font_scale=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Demo of unicode support in text and labels.\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.datasets import load_svmlight_file, load_boston, load_breast_cancer\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_boston, load_diabetes, load_linnerud, make_regression\n",
    "from sklearn.datasets import make_friedman1, make_friedman2, make_friedman3, make_sparse_uncorrelated\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples, completeness_contamination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Ograniczenia modeli\n",
    "* modele liniowe mają ograniczenia z powodu klątwy wymiarowości\n",
    "* niewystarczające są __ustalone__ funkcje bazowe\n",
    "  * zły wybór może powodować overfitting  \n",
    "  * trzeba __adaptować__ funkcje do konkretnych danych\n",
    "  * ograniczyć liczbę funkcji bazowych\n",
    "\n",
    "* alternatywne podejście\n",
    "  * ustalić funkcje bazowe z góry - niewielką liczbę, zwykle __jedną__\n",
    "  * niech funkcje __adaptują__ się w trakcie nauki\n",
    "  * model jest zwykle znacznie mniejszy\n",
    "  * funkcja ___likelihood_ nie będzie już wypukła__\n",
    "    * konieczność przeglądnięcia większej liczby modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron biologiczny\n",
    "\n",
    "  <img src=\"../mum_figures/neuron.png\" width=\"100%\"/>\n",
    "  \n",
    "  <img src=\"../mum_figures/synapse.jpg\" width=\"100%\"/>\n",
    "\n",
    "![Potential](file:///Users/igorpodolak/Dropbox/mum_figures/potential.gif)\n",
    "\n",
    "![Awesome cat gif](http://media.giphy.com/media/7z2oyDXIMEs8w/giphy.gif)\n",
    "\n",
    "![Potential](http://github.com/gmum/ml2017/wyklady/mum_figures/potential.gif)\n",
    "\n",
    "<img src=\"../mum_figures/potential.jpg\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci z propagacją sygnału (feed-forward)\n",
    "<img src=\"../mum_figures/mlp.png\" width=\"100%\"/>\n",
    "* funkcje bazowe modelu\n",
    "  * muszą __zależeć__ od parametrów\n",
    "  * parametry powinny się zmieniać w trakcie uczenia\n",
    "  * współczynniki $w_j$ także są modyfikowalne\n",
    "* $M$ liniowych kombinacji wejść $$v_j=\\sum_i^Dw_{ji}x_i+w_{j0}$$\n",
    "gdzie $M$ jest liczbą __\"neuronów\"__ w pierwszej __warstwie__ modelu\n",
    "  * $w_{j0}$ to __bias__ $j$-tego neuronu\n",
    "  * $v_j$ to __aktywacje__\n",
    "  * teraz użyta jest __funkcja aktywacji__ $\\varphi()$ (różniczkowalna) $$z_j=\\varphi(v_j)$$\n",
    "    * funkcje aktywacji są zwykle funkcjami sigmoidalnymi, np.\n",
    "    $$\\frac{1}{1+\\exp({-v})}, \\tanh(-v)$$\n",
    "  * neurony w warstwach, które nie są ani wejściową ani ukrytą nazywamy __ukrytymi__\n",
    "  \n",
    "  \n",
    "* obliczenia powtarzają się w kolejnych warstwach\n",
    "* końcowa wartość jest jako, dla sieci z jedną warstwą ukrytą\n",
    "$$y_k(x)=\\sigma\\left(\\sum_k^Mw_kj\\sigma\\left(\\sum_i^Dw_{ji}x_i+w_{j0}\\right)+w_{k0}\\right)$$\n",
    "  * jeśli problem regresji, to końcowa funkcja aktywacji jest identycznością\n",
    "  * sieć neuronowa jest __nieliniową__ funkcją wejść $x_i$\n",
    "    * wartość bias włączamy do wektora $w$ __rozszerzając__ wejście o $x_0=1$\n",
    "\n",
    "* sygnał jest przesyłany do przodu, stąd nazwa\n",
    "* sieć wielowarstwowa nazywana __wielowarstwowym perceptronem__\n",
    "  * zaproponowany przez McCullocha i Pittsa __perceptron__ \n",
    "    * wykorzystywał __progową__ funkcję aktywacji (f. Heaviside'a)\n",
    "    * nie zawierał warstw ukrytych\n",
    "\n",
    "* sieć z __liniowymi__ f. aktywacji w w. ukrytych jest redukowalna do sieci __bez__ warstw ukrytych\n",
    "* sieci warstwowe można uogólnić przez dodanie __połączeń skrótowych__\n",
    "\n",
    "* nazewnictwo\n",
    "  * istnieje wiele zwyczajów: sieć na rysunku może być nazywana \n",
    "    * 4-warstwową (bo liczba wszystkich warstw neuronów), \n",
    "    * 3-warstwową (bo liczba warstw adaptowalnych parametrów), \n",
    "    * 2-warstwową (bo liczba warstw ukrytych)\n",
    "  * Bishop preferuje drugie podejście\n",
    "  * jasne będzie \"sieć z dwoma warstwami ukrytymi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci neuronowe jako uniwersalny aproksymator\n",
    "> Sieć neuronowa z jedną warstwą ukrytą neuronów z nieliniowymi ciągłymi monotonicznie rosnącymi funkcjami aktywacji może dowolnie dokładnie dla $\\epsilon>0$ aproksymować dowolną funkcję ciągła zdefiniowaną na hiperkostce $[0, 1]^m$.\n",
    "\n",
    "> Twierdzenie jest o _istnieniu_ takiej sieci, a nie mówi jak znaleźć odpowiednią liczbę neuronów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja kosztu: regresja\n",
    "* niech przewidywana pojedyncza zmienna $t$ ma rozkład normalny zależny od wejściowego $x$ $$p(t\\mid x)=\\mathcal{N}(t\\mid y(x, w), \\sigma)$$ gdzie\n",
    "  * $y(x, w)$ jest wyjściem sieci\n",
    "  * $\\sigma$ jest wariancją\n",
    "* $X=\\{x_1,x_2,\\dots,x_N\\}$ zbiorem $N$ __niezależnych__ przykładów\n",
    "* dla wartości spodziewanych (target) $\\{t_1,\\dots,t_N\\}$ można utworzyć __likelihood__ \n",
    "$$p(t\\mid X,w,\\sigma)=\\prod_{n=1}^Np(t_n\\mid x_n,w,\\sigma)$$\n",
    "\n",
    "* biorąc __ujemny logarytm__ dostajemy __log-likelihood__\n",
    "$$\\frac{1}{2\\sigma}\\sum_{n=1}^N\\left(y(x_n,w)-t_n\\right)^2+\\frac{N}{2}\\ln\\sigma +\\frac{N}{2}\\ln(2\\pi)$$\n",
    "* maksymalizacja likelihood jest równoważna __minimalizacji__ log-likelihood, co jest identyczne z minimalizacją funkcji kosztu\n",
    "$$E(w)=\\frac{1}{2}\\sum_{n=1}^N\\left(y(x_n,w)-t_n\\right)^2$$\n",
    "  * nieliniowość funkcji $y(x,w)$ powoduje, że $E(w)$ __nie jest wypukła__\n",
    "  * funkcja kosztu będzie miała __minima lokalne__\n",
    "  * po znalezieniu wektora $w^\\ast$ można znaleźć $\\sigma$ jako\n",
    "  $$\\sigma=\\frac{1}{N}\\sum_{n=1}^N\\left(y(x_n,w^\\ast)-t_n\\right)^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja kosztu: klasyfikacja\n",
    "* przypadek klasyfikacji binarnej\n",
    "  * pojedynczy neuron wyjsciowy z $\\sigma()$ __logistyczną__\n",
    "  $$y=\\sigma(v)=\\frac{1}{1+\\exp(-v)}$$\n",
    "  * niech $t=1$ jeśli $x\\in C_1$; $t=2$ jeśli $x\\in C_2$\n",
    "  * niech $y(x,w)$ będzie __prawdopodobieństwem__, że $x\\in C_1$\n",
    "  * $1-y(x,w)$ prawdopodobieństwem, że $x\\in C_2$\n",
    "  \n",
    "* rozkład warunkowy wyjść rozkładem Bernoulliego\n",
    "$$p(t\\mid x)=y(x,w)^t\\left(1-y(x,w)\\right)^{1-t}$$\n",
    "\n",
    "* dla zbioru wszystkich przykładów $X$ mamy likelihood\n",
    "$$p(t\\mid X,w)=\\prod_ny(x_n,w)^t_n\\left(1-y(x_n,w)\\right)^{1-t_n}$$\n",
    "* znowu ujemny logarytm daje log-likelihood i funkcję kosztu\n",
    "$$E(w)=-\\sum_{n=1}^N\\left[t_n\\ln\\,y_n+(1-t_n)\\ln\\,(1-y_n)\\right]$$\n",
    "nazywaną __entropią krzyżową__\n",
    "  \n",
    "  * dla klasyfikacji użycie entropii krzyżowej daje lepsze wyniki niż użycie funkcji kwadratowej kosztu\n",
    "* dla problemu wieloklasowego $t\\in\\{C_1,\\dots,C_K\\}$ mamy\n",
    "$$E(w)=-\\sum_{n=1}^N\\sum_{k=1}^K\\left[t_{nk}\\ln\\,y_k(x_n,w)+(1-t_{nk})\\ln\\,(1-y_k(x_n,w))\\right]$$\n",
    "* dla klasyfikacji wieloklasowej mamy\n",
    "  $$y_k(x,w)=p(t_k=1\\mid x)$$\n",
    "  * to skraca wyrażenie entropii krzyżowej do\n",
    "  $$E(w)=-\\sum_{n=1}^N\\sum_{k=1}^Kt_n\\ln\\,y_k(x_n,w)$$\n",
    "  * dla problemu wieloklasowego aktywacją wyjściową jest funkcja softmax\n",
    "  $$y_k(x,w)=\\frac{\\exp(v_k(x,w))}{\\sum_{j=1}^K\\exp(v_j(x,w))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cechy\n",
    "* podstawowa różnica do modeli liniowych: sieć neuronowa __mapuje przestrzeń wejściową do przestrzeni ukrytej gdzie znajduje rozwiązanie__\n",
    "  * __nieliniowe__ rzutowanie do przestrzeni ukrytej zwiększa prawdopodobieństwa liniowej separowalności\n",
    "\n",
    "* aktywacje warstw ukrytych dla danego wejścia nazywamy cechami\n",
    "  * w modelach każda przynależność do klasy jest rozwiązywana jako __całkowicie osobny__ problem\n",
    "  * w sieciach podproblemy przynależności do klas __dzielą cechy__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie sieci warstwowych neuronowych\n",
    "* zadaniem jest minimalizacja $E(w)$\n",
    "* drobna zmiana $w$ powoduje __presunięcie__ się po powierzchni funkcji kosztu\n",
    "  * dualność: każde rozwiązanie odpowiada punktowi w przestrzeni wag\n",
    "* przestrzeń rozwiązań jest zwykle bardzo wysoko wymiarowa\n",
    "  * istnieje wiele punktów o $\\nabla{}E(w)\\simeq0$\n",
    "  * to minima/maksima lokalne/globalne, punkty siodłowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lokalna aproksymacja kwadratowa\n",
    "* rozwinięcie funkcji kosztu w szereg Taylora\n",
    "$$E(w)\\simeq E(w_0)+(w-w_0)g+\\frac{1}{2}(w-w_0)H(w-w_0)$$\n",
    "gdzie \n",
    "$$\\begin{align}\n",
    "g=&\\frac{\\partial E}{\\partial w}\\\\\n",
    "H=&\\frac{\\partial^2 E}{\\partial w_i\\,\\partial w_j}\n",
    "\\end{align}$$\n",
    "\n",
    "* stąd\n",
    "$$\\nabla E\\simeq g+H(w-w_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie gradientowe\n",
    "* uczenie małymi krokami\n",
    "$$w(t+1)=w(t)-\\eta\\nabla E(w(t))$$\n",
    "gdzie $\\eta$ jest __współczynnikiem nauczania__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tryby uczenia gradientowego\n",
    "\n",
    "* to metoda __epoka po epoce__ (ang. batch)\n",
    "  * każdy krok poprawy wymaga wielu obliczeń\n",
    "  * każda propagacja wprzód używa starych wag\n",
    "  * wiele przykładów dzieli wspólne cechy, stąd utrata wielu możliwych informacji\n",
    "  * uczenie po epokach może powodować krążenie w przestrzeni rozwiązań\n",
    "  * usprawnienie: metoda gradientów sprzężonych zapewniająca poprawy\n",
    "    * każda poprawa jest ortogonalna do poprzednich\n",
    "    * teoretycznie roziązanie po co najwyżej tylu krokach ile jest wag\n",
    "\n",
    "* epoka __przykład po przykładzie__ (ang. stochastic gradient descent)\n",
    "$$E(w)=\\sum_nE_n(w)$$\n",
    "  * poprawa po każdym przykładzie\n",
    "  $$w(t+1)=w(t)-\\eta\\nabla E_n(w(t))$$\n",
    "  * uwzględnia wiedzę z innych przykładów od razu\n",
    "  * pozwala na łatwiejsze omijania minimów lokalnych\n",
    " \n",
    "* __mini batch__\n",
    "  * uczenie małymi grupami przykładów\n",
    "  * najbardziej efektywne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wsteczna propagacja błędów\n",
    "* sieci przechodziły przez wzloty i upadki\n",
    "* zapowiadano (Minsky i Papert w _Perceptron_) śmierć ze względu na __niemożność__ uczenia sieci z warstwami ukrytymi\n",
    "* problem z liczeniem gradientu dla warstw ukrytych\n",
    "* algorytm wstecznej propagacji zaproponowany przez Werbosa\n",
    "\n",
    "\n",
    "* w trakcie uczenia obliczana jest duża liczba pochodnych funkcji kosztu __względem__ wag\n",
    "* wsteczna propagacja jest __efektywnym__ machenizmem ich obliczania\n",
    "  * pochodne (błędy) są przesyłane wstecz w sieci\n",
    "* drugim elementem jest obliczenie i modyfikacja wartości wag\n",
    "\n",
    "* schemat może być użyty do szerokiego wachlarza architektur sieci\n",
    "\n",
    "\n",
    "\n",
    "Funkcja kosztu $$E(w)=\\sum_{n=1}^NE_n(w)$$\n",
    "<img src=\"../mum_figures/backpropagation.png\" width=\"70%\"/>\n",
    "1. neurony wyjściowe w modelu __liniowym__\n",
    "  * $y_k=\\sum_kw_{ki}x_i$ i funkcją kosztu \n",
    "  $$E_n=\\frac{1}{2}\\sum_k(y_{nk}-t_{nk})^2$$\n",
    "  * uczenie polega na __modyfikacji__ wag w celu __minimalizacji__ kosztu\n",
    "  \n",
    "  $$\\frac{\\partial E_n}{\\partial w_{kj}}=(y_{nj}-t_{nj})x_i$$\n",
    "  co jest uczeniem __przez poprawianie błędu__\n",
    "2. modele wielowarstwowe\n",
    "  * aktywacja neuronu w warstwie to\n",
    "  \n",
    "  $$z_j=\\varphi(v_j)=\\varphi\\left(\\sum_iw_{ji}z_i\\right)$$\n",
    "  z nieliniową funkcją aktywacji $\\varphi(\\cdot)$\n",
    "  * koszt jest zależny od wag __tylko__ przez sumę ważoną $v_j$\n",
    "  \n",
    "  $$\\frac{\\partial E_n}{\\partial w_{ji}}=\\frac{\\partial E_n}{\\partial v_j}\\frac{\\partial v_j}{\\partial w_{ji}}$$\n",
    "  <img src=\"../mum_figures/backpropagation.png\" width=\"80%\"/>\n",
    "  * wprowadzamy oznaczenie __delta__\n",
    "  \n",
    "  $$\\delta_j\\equiv\\frac{\\partial E_n}{\\partial v_j}$$\n",
    "  odpowiadające lokalnemu wpływowi na całkowity błąd\n",
    "  \n",
    "  $$\\begin{align}\n",
    "  \\frac{\\partial E_n}{\\partial w_{ji}}&=\\frac{\\partial E_n}{\\partial v_j}\\frac{\\partial v_j}{\\partial w_{ji}}\\\\\n",
    "  &=\\delta_j\\frac{\\partial v_j}{\\partial w_{ji}}\\\\\n",
    "  &=\\delta_jz_i\n",
    "  \\end{align}$$\n",
    "    * dla neuronów (liniowych) __wyjściowych__ mamy\n",
    "    \n",
    "    $$\\delta_k=\\frac{\\partial E_n}{\\partial v_k}=y_k-t_k$$\n",
    "    * dla neuronów __ukrytych__ (tu w warstwie __poprzedzającej__ wyjściową)\n",
    "    \n",
    "    $$\\begin{align}\n",
    "    \\delta_j&=\\frac{\\partial E_n}{\\partial v_j}\\\\\n",
    "    &=\\sum_k\\frac{\\partial E_n}{\\partial v_k}\\frac{\\partial v_k}{\\partial v_j}\\\\\n",
    "    &=\\sum_k\\delta_k\\frac{\\partial v_k}{\\partial v_j}\\\\    &\\hskip{4em}\\text{gdzie}\\hskip{1em}v_k=\\sum_jw_{kj}z_j=\\sum_jw_{kj}\\varphi(v_j)\\\\\n",
    "    &=\\varphi'(v_j)\\sum_kw_{kj}\\delta_k\n",
    "    \\end{align}$$\n",
    "    <img src=\"../mum_figures/backpropagation.png\" width=\"100%\"/>\n",
    "      * wartości $\\delta_k$ zostały już obliczone dla wartwy __późniejszej__ w modelu sieci\n",
    "      * $\\delta_j$ odpowiadają __wpływowi $j$-tego neuronu na błędy w późniejszych warstwach__, czyli __loklanemu błędowi $j$-tego ukrytego neuronu__\n",
    "      * stąd __wsteczna propagacja błędów__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje aktywacji\n",
    "* zwykle __sigmoidalne__\n",
    "  * wprowadzają __nieliniowość__\n",
    "  * __logistyczna__ $$\\sigma(v)=\\frac{1}{1+exp(-v)}$$\n",
    "    * pochodna prosta obliczeniowo $$\\sigma'(v)=\\sigma(v)(1-\\sigma(v))$$\n",
    "  * __tangens hiperboliczny__\n",
    "    * aktywacja na $(-1, 1)$\n",
    "    * preferowany dla niskich warstw\n",
    "      * pozwala by wymusić średnią aktywacji $0$ co zwykle przyspiesza uczenie\n",
    "  * gradient dla wysokich wartości bezwzględnych __zanika__\n",
    "    * tzw. __vanishing gradient problem__\n",
    "    * uczenie zanika mimo błędu\n",
    "* problem zanikającego gradientu utrudnia uczenie sieci o wielu warstwach ukrytych\n",
    "  * propozycja aktywacji __rectified linear__ $$\\varphi(v)=\\max(0, v)$$\n",
    "    * gradient nie zanika\n",
    "    * możliwe modyfikacje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co się dzieje w warstwie ukrytej (3 różne klasy)\n",
    "\n",
    "* aktywacje oczekiwne\n",
    "<img src=\"../mum_figures/cl3-in.png\" width=\"50%\"/>\n",
    "* aktywacje uzyskane\n",
    "<img src=\"../mum_figures/cl3-out.png\" width=\"50%\"/>\n",
    "* aktywacje warstwy ukrytej (kolory według klasy uzyskanej)\n",
    "<img src=\"../mum_figures/cl3-hid.png\" width=\"50%\"/>\n",
    "* błąd aktywacji\n",
    "<img src=\"../mum_figures/cl3-err.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co się dzieje w warstwie ukrytej (4 różne klasy)\n",
    "\n",
    "* aktywacje oczekiwne\n",
    "<img src=\"../mum_figures/cl4-in.png\" width=\"50%\"/>\n",
    "* aktywacje uzyskane\n",
    "<img src=\"../mum_figures/cl4-out.png\" width=\"50%\"/>\n",
    "* aktywacje warstwy ukrytej (kolory według klasy uzyskanej)\n",
    "<img src=\"../mum_figures/cl4-hid.png\" width=\"50%\"/>\n",
    "* błąd aktywacji\n",
    "<img src=\"../mum_figures/cl4-err.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czy wsteczna propagacja jest efektywna obliczeniowo?\n",
    "* liczba obliczeń przy obliczaniu aktywacji jest proporcjonalna do liczby wag\n",
    "  * wag jest __znacznie__ więcej niż neuronów\n",
    "* obliczenia dla funkcji kosztu też są rzędu liczby wag\n",
    "\n",
    "* numeryczne obliczanie pochodnych \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w}=\\frac{E(w+\\epsilon)-E(w)}{\\epsilon}+O(\\epsilon)$$\n",
    "lub\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w}=\\frac{E(w+\\epsilon)-E(w-\\epsilon)}{2\\epsilon}+O(\\epsilon^2)$$\n",
    "ma wyższą złożoność $O(w^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obliczenia drugiego rządu\n",
    "* macierz drugich pochodnych\n",
    "\n",
    "$$\\frac{\\partial^2 E}{\\partial w_j \\partial w_k}$$\n",
    "* macierz Hesjanu $$H=\\nabla\\nabla E$$\n",
    "  * pozwala w optymalizacju brać pod uwagę __krzywiznę__ powierzchni błędu\n",
    "    * gradient tylko kierunek zmian\n",
    "  * odwrotność H umożliwia znalezienie nieistotnych wag w sieci (pruning)\n",
    "  * szybkie douczanie po zmianie przykładów\n",
    "    * niewielka zmiana położenia minimum\n",
    "* wyższa złożoność $O(W^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularyzacja\n",
    "* zwykle bardzo duża liczba parametrów (wag)\n",
    "* konieczność długich testów dla znalezienia optymalnych parametrów\n",
    "  * liczba neuronów, wartości wag, etc.\n",
    "\n",
    "1. __weight decay__ (__L2__)\n",
    "\n",
    "$$\\overline{E}(w)=E(w)+\\frac{1}{2}w^Tw$$\n",
    "\n",
    "2. __L1__\n",
    "$$\\overline{E}(w)=E(w)+\\|w\\|_1$$\n",
    "\n",
    "2. problemy z liniową transformację\n",
    "  * skalowanie wejść jest __niespójne__ z prostą regularyzacją __ weight decay__\n",
    "    * translacja (skalowanie) wejść (lub wyjść) da nierównoważną (przez liniowe przekształcenie) sieć przy uczeniu z weight decay\n",
    "    * translacje inaczej wpływają na różne warstwy sieci\n",
    "  * osobny czynnik egularyzacyjny dla osobnych warstw sieci\n",
    "  \n",
    "  $$\\frac{\\lambda_1}{2}\\sum_{w\\in W_1}w^2+\\frac{\\lambda_2}{2}\\sum_{w\\in W_2}w^2$$\n",
    "  \n",
    "3. wczesne zatrzymywanie\n",
    "  * błąd na zbiorze uczącym maleje w trakcie uczenia\n",
    "  * błąd na zbiorze __walidującym__ zaczyna rosnąć\n",
    "    * jest to znak rozpoczęcia zbytniego dopasowania\n",
    "  <img src=\"../mum_figures/generalization.png\" width=\"120%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatywy\n",
    "* wsteczna propagacja _per se_ jest powolna\n",
    "* blisko minimum może wpaść w oscylacje\n",
    "* problemy przy niskim spadku\n",
    "* metoda __momentum__\n",
    "\n",
    "$$w(t+1)=w(t)-\\eta\\nabla E(t) -\\alpha\\Delta(w(t-1))$$\n",
    "  * szereg usprawnień, np. algorytm Adam\n",
    "* metody adaptacynego kroku\n",
    "  * atrybuty wejsciowe nie mają równego wpływu na rozwiązanie\n",
    "  * osobne parametry uczenia dla każdego z nich\n",
    "  * algorytm delta-delta\n",
    "* problemy z _płaską_ powierzchnią błędu\n",
    "  * metoda __resilient backpropagation__\n",
    "  \n",
    "  $$\\Delta w_{ji}(t+1)=\\begin{cases}\n",
    "  -\\eta_{ji}sgn(\\nabla_{ji} E(t))&\\text{jeśli}\\;\\;\\nabla_{ji}E(t)\\nabla_{ji}E(t-1)\\geq0\\\\\n",
    "  0; \\nabla_{ji}E(t)=0&\\text{w przeciwnym przypadku}\n",
    "  \\end{cases}$$\n",
    "  * parametr też potrzebuje być modyfikowany\n",
    "  $$\\eta_{ji}(t+1)=\\begin{cases}\n",
    "  min(u*\\eta_{ji}, \\eta_{max})&\\nabla_{ji}E(t)\\nabla_{ji}E(t-1)>0\\\\\n",
    "  max(d*\\eta_{ji}, \\eta_{min})&\\nabla_{ji}E(t)\\nabla_{ji}E(t-1)<0\\\\\n",
    "  \\eta_{ji}&\\text{wpp}\n",
    "  \\end{cases}$$\n",
    "  dla $u>1$ i $d<1$, np. $u=1.05$ i $d=0.7$\n",
    "    * podwyższany jeśli kolejne gradienty w tym samym kierunku\n",
    "    * zmniejszany jeśli w przeciwnym\n",
    "* bardzo efektywne podejście dla algorytmów klasyfikacji\n",
    "\n",
    "* wiele obecnych algorytmów, np. Adam, współdzielą wiele cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci konwolucyjne\n",
    "* wiele cech jest dzielonych\n",
    "* mogą wystąpić w __różnych__ częściach wejścia (obrazy)\n",
    "* konieczna niezależność (chociaż nawet niewielka) na translację/skalowanie\n",
    "  1. utworzyć bardzo duży zbiór uczący wszelkich możliwości\n",
    "  \n",
    "  \n",
    "  \n",
    "* lokalne cechy\n",
    "  * niewielkie obszary wejściowe\n",
    "  * filtry/konwolucje\n",
    "  * struktura kratki\n",
    "    * jedno-wymiarowe szeregi czasowe\n",
    "    * dwu-wymiarowe obrazy\n",
    "    * czasem trój-wymiarowa (obrazy medyczne)\n",
    "* dzielenie wag\n",
    "  * te same cechy w różnych obszarach\n",
    "  * ograniczenie liczby rzeczywistych parametrów\n",
    "* subsampling\n",
    "  * odporność na translacje\n",
    "* hierarchiczność\n",
    "  * wykrywanie prostych cech i ich składanie\n",
    "  \n",
    "  \n",
    "* model Neocognitron Fukushimy (1980)\n",
    "  <img src=\"../mum_figures/neocognitron.png\" width=\"120%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet (ImageNet, KSH) (Krizhevsky, Sutskeyer, Hinton)\n",
    "<img src=\"../mum_figures/alexnet.jpg\" width=\"120%\"/>\n",
    "\n",
    "<img src=\"../mum_figures/conv-images.png\" width=\"120%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konwolucje\n",
    "1. jedno-wymiarowe\n",
    "  * pomiar danych w których ostatnie odczyty są bardziej znaczące (szeregi czasowe)\n",
    "  \n",
    "  $$s(t)=\\int x(a)w(t-a)da$$\n",
    "  jest __konwolucją__ gdzie $a$ jest wiekiem pomiaru, zwykle $s(t)=(x\\ast w)(t)$\n",
    "    * $x$ to __wejście__\n",
    "    * $w$ __jądro__ (kernel)\n",
    "    * wyjście jest __cechą__\n",
    "  * pomiary są dyskretne\n",
    "  \n",
    "  $$s(t)=\\sum_{a=-\\infty}^\\infty x(a)w(t-a)$$\n",
    "    * wartości jądra są równe $0$ poza skończoną liczbą pozycji\n",
    "2. dwu-wymiarowe\n",
    "  * filtr na obrazie $I$\n",
    "  \n",
    "  $$\\begin{align}\n",
    "  s(i,j)&=(I\\ast K)(i, j)=\\sum_m\\sum_n I(m,n)K(i-m,j-n)\\\\\n",
    "  &=(K\\ast I)(i, j)=\\sum_m\\sum_n I(i-m,j-n)K(m,n)\n",
    "  \\end{align}$$\n",
    "  * zwykle implementowane jest __korelacja krzyżowa__, równoważna\n",
    "  \n",
    "  $$s(i,j)=(K\\ast I)(i, j)=\\sum_m\\sum_n I(i+m,j+n)K(m,n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cechy\n",
    "* rzadkie połączenia, rzadkie wagi\n",
    "  * kernele znacznie mniejsze od obrazu\n",
    "    * efektywność obliczeniowa\n",
    "  * pojedyncze wejścia wpływają tylko na niewiele wyjść\n",
    "  * na pojedynczą konwoulcję (wyjście) wpływa niewiele wejść\n",
    "* lokalność cech\n",
    "  * konwolucje wyszukują lokalne cechy\n",
    "  * pozwala na pozbycie się kontekstu\n",
    "  <img src=\"../mum_figures/pixels_embed_cifar10.jpg\" width=\"100%\"/>\n",
    "  \n",
    "* dzielenie wag\n",
    "  * tradycyjnie jedna waga między parą wejście-wyjście\n",
    "  * konwolucje pozwalają na dzielenie wag\n",
    "  * zmniejszenie liczby parametrów i wielkości pamięci\n",
    "  * pozwala na uniezależnienie od translacji\n",
    "    * te same cechy wykrywane w szeregach czasowych w różnych okresach\n",
    "    * te same filtry aktywowane na obrazach\n",
    "    \n",
    "* konwolucje w niskich warstwach wykrywają zwykle brzegi, krawędzie\n",
    "  * to pozwala na tzw. __transfer learning__\n",
    "  \n",
    "* __padding__ to operacja __poszerzania__ wejścia o zerowe wejścia na brzegu\n",
    "  * możliwe na każdym poziomie\n",
    "  * w przeciwnym wypadku wymiar zmniejsza się\n",
    "    * dla wejścia o szerokości $m$ i jądrze o szerokości $k$ wyjście będzie miało $m-k+1$ pikseli\n",
    "      * bardzo szybkie zmniejszanie dla duzych kerneli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "* odporność na __lokalne translacje__\n",
    "* wybór z ograniczonego obszaru najsilniejszej (max-pooling) aktywacji\n",
    "  * odpowiada wybraniu cechy najsilniejszej w obszarze\n",
    "  * istotne znalezienie cechy bez dokładnego określenia położenia\n",
    "* pooling zwykle ogranicza wielkość obszaru\n",
    "  * uogólnianie cech\n",
    "  \n",
    "* czasem proponowane jest opuszczenie warstw pooling\n",
    "  * propozycje użycia konwolucji z większym przesunięciem (ang. stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
